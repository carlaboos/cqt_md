{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmiSw+N4SJCtpP5f/Gl1x0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Extract questions for manual data annotation"],"metadata":{"id":"7h0qAT02ADT2"}},{"cell_type":"markdown","source":["---\n","* Load data\n","\n","---"],"metadata":{"id":"Fz6CQHWltGYJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6twQkBFWoIZv"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","file_name = \"cleaned_dataset.csv\"\n","data = pd.read_csv(file_name)\n","\n","# Progress Check-In\n","print(\"\\n=== Initial Data Summary ===\")\n","print(f\"Dataset: {len(data)} rows, columns: {data.columns.tolist()}\")\n","\n","# Check the first few rows to confirm structure\n","print(data.head())"]},{"cell_type":"markdown","source":["---\n","* Create progress check-in\n","* Create extraction of a sample set for review after each step\n","\n","---"],"metadata":{"id":"GQpTtp8xaygU"}},{"cell_type":"code","source":["# Logging functions for progress check-ins\n","def log_data_summary(data, step_name):\n","    print(f\"\\n=== Summary After Step: {step_name} ===\")\n","    print(f\"Number of rows: {len(data)}\")\n","    print(f\"Number of duplicate rows (based on 'question_text'): {data.duplicated(subset='question_text').sum()}\")\n","    print(f\"Number of empty rows in 'question_text': {data['question_text'].isnull().sum()}\")\n","    print(f\"Sample of 'question_text':\\n{data['question_text'].head(5)}\")\n","\n","def save_sample(data, step_name, sample_size=50):\n","    sample = data.sample(sample_size, random_state=42)\n","    sample.to_csv(f'sample_after_{step_name}.csv', index=False, encoding='utf-8')\n","    print(f\"Sample saved for step: {step_name}\")\n","\n","# Check-in\n","log_data_summary(data, \"Initial Load\")\n","save_sample(data, \"initial_load\")"],"metadata":{"id":"ZEKTgVuuayvs","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","* Initialize spaCy for word class detection\n","\n","---"],"metadata":{"id":"8ZE4MY00jH6m"}},{"cell_type":"code","source":["!python -m spacy download de_core_news_sm"],"metadata":{"id":"5OzbKYFyjtAU","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load SpaCy German language model\n","nlp = spacy.load(\"de_core_news_sm\")\n","\n","# Function to check if a word is a noun\n","def is_noun(word):\n","    doc = nlp(word)\n","    return any(token.pos_ == \"NOUN\" for token in doc)\n"],"metadata":{"id":"Yo7TrgrNjGhs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","Step 4\n","\n","* Protect patterns in the text\n","* Apply rules for handling periods that should not be interpreted as sentence-ending.\n","\n","---"],"metadata":{"id":"W3b54pDWtOLW"}},{"cell_type":"code","source":["import re\n","from tqdm import tqdm\n","\n","# Initialize tqdm for pandas\n","tqdm.pandas()\n","\n","# List of abbreviations\n","abbreviations = {\n","    \"Abb\", \"Abk\", \"Abs\", \"Abschn\", \"Adj\", \"Adv\", \"afk\", \"afr\", \"AJ\", \"Akk\", \"allg\", \"alph\", \"Alt\",\n","    \"Anh\", \"Anl\", \"Anm\", \"Apr\", \"arab\", \"Art\", \"Aufl\", \"Aug\", \"Ausg\", \"Az\", \"Bd\", \"BL\", \"bes\", \"Bez\",\n","    \"BHF\", \"Bsp\", \"bspw\", \"bzgl\", \"bzw\", \"ca\", \"ccm\", \"cm\", \"CPU\", \"Dat\", \"dazw\", \"ders\", \"desgl\",\n","    \"Det\", \"Dez\", \"dgl\", \"dh\", \"Di\", \"Do\", \"Dr\", \"dt\", \"Dt\", \"EG\", \"ehem\", \"eigntl\", \"eigtl\", \"Eings\",\n","    \"einschl\", \"entsp\", \"erg\", \"etc\", \"ETF\", \"etw\", \"ev\", \"evtl\", \"f\", \"Feb\", \"FC\", \"Fem\", \"ff\", \"Fr\",\n","    \"Fr\", \"Frl\", \"FS\", \"Fut\", \"g\", \"geb\", \"gegr\", \"gem\", \"Gen\", \"gesch\", \"gf\", \"ggf\", \"ggs\", \"ggü\", \"GK\",\n","    \"GPU\", \"grds\", \"HBF\", \"HJ\", \"Hr\", \"HS\", \"iA\", \"idR\", \"idS\", \"Imp\", \"inkl\", \"insb\", \"inzw\", \"iÜ\",\n","    \"Jan\", \"Jh\", \"Jhdt\", \"Jhs\", \"jmd\", \"jmdm\", \"jmdn\", \"jmds\", \"Jul\", \"Jun\", \"jährl\", \"KA\", \"Kal\",\n","    \"Kap\", \"kath\", \"kg\", \"KH\", \"km\", \"kompl\", \"Konj\", \"kw\", \"led\", \"LAN\", \"LH\", \"LJ\", \"LK\", \"m\",\n","    \"Mai\", \"Mask\", \"maW\", \"max\", \"mE\", \"med\", \"Mi\", \"min\", \"Mio\", \"mM\", \"mm\", \"mMn\", \"Mo\", \"Mr\", \"Mrd\",\n","    \"Mrs\", \"Ms\", \"mtl\", \"mW\", \"Mwst\", \"Mär\", \"Neutr\", \"Nom\", \"Nov\", \"Nr\", \"NS\", \"oa\", \"og\", \"Okt\",\n","    \"oä\", \"OG\", \"OP\", \"P\", \"PC\", \"Part\", \"PDF\", \"Pers\", \"PF\", \"PK\", \"Pkt\", \"Pl\", \"Pos\", \"Poss\", \"Pron\",\n","    \"Präp\", \"RAM\", \"Rel\", \"resp\", \"RGB\", \"Sa\", \"Sachgeb\", \"sek\", \"Sep\", \"Sg\", \"Sing\", \"So\", \"SS\", \"SSD\",\n","    \"SST\", \"SSW\", \"St\", \"std\", \"Tab\", \"taus\", \"Tel\", \"trad\", \"Tsd\", \"tägl\", \"ua\", \"UG\", \"ugs\",\n","    \"unverh\", \"urspr\", \"usw\", \"uU\", \"uvm\", \"uä\", \"va\", \"verh\", \"verw\", \"vgl\", \"vh\", \"vllt\", \"vlt\",\n","    \"vs\", \"Wdh\", \"wg\", \"WK\", \"zb\", \"Ziff\", \"zit\", \"zT\", \"zz\", \"zzgl\", \"zzzt\"\n","}\n","\n","# List of URL suffixes\n","url_suffixes = {\"com\", \"net\", \"org\", \"info\", \"biz\", \"de\", \"co\", \"uk\", \"us\", \"fr\", \"es\", \"it\", \"ru\", \"cn\",\n","                \"au\", \"ca\", \"jp\", \"in\", \"br\", \"za\", \"kr\", \"mx\", \"tech\", \"io\", \"xyz\", \"online\", \"store\",\n","                \"site\", \"me\", \"ai\", \"dev\"}\n","\n","# Regex for abbreviations\n","abbreviation_regex = r'\\b(?:' + '|'.join(re.escape(abbrev) for abbrev in abbreviations) + r')\\s*\\.'\n","\n","# Function to protect patterns in text\n","def protect_patterns(text):\n","    # Rule 1: Special case for \"gutefrage . net\" (case-insensitive)\n","    #text = re.sub(r'\\bgutefrage\\s\\.\\snet\\b', r'gutefrage[PROTECT]net', text, flags=re.IGNORECASE)\n","\n","    # Rule 2: Protect numbers in enumerations with unlimited items (1 . zuerst das 2 . dann das 3 . dann das)\n","    text = re.sub(r'((?:\\d+\\s\\.\\s[^\\d.]+)+)\\s\\d+\\s\\.\\s', r'\\1[PROTECT] ', text)\n","\n","    # Rule 3: Protect ordinal numbers followed by nouns (2 . Staffel, 10. Hochzeitstag)\n","    text = re.sub(r'(\\d+)\\s\\.\\s(\\w+)', lambda m: f\"{m.group(1)}[PROTECT]{m.group(2)}\" if is_noun(m.group(2)) else m.group(0), text)\n","\n","    # Rule 4: Protect numbers followed by closed brackets (1 . ) = enumerations)\n","    text = re.sub(r'(\\d+)\\s\\.\\s*\\)', r'\\1[PROTECT])', text)\n","\n","    # Rule 5: Protect ordinal numbers followed by acronyms (3 . OG)\n","    text = re.sub(r'(\\d+)\\s\\.\\s([A-ZÄÖÜ]{2,})', r'\\1[PROTECT]\\2', text)\n","\n","    # Rule 6: Protect abbreviations\n","    def abbreviation_handler(match):\n","        # Number preceding abbreviation, if any\n","        preceding_number = match.group(1)\n","        # Abbreviation (e.g., etc)\n","        abbreviation = match.group(2)\n","        # Word following the abbreviation\n","        next_word = match.group(3)\n","\n","        # If preceded by a number, treat as non-sentence-ending (3 . Abs .)\n","        if preceding_number:\n","            return f\"{preceding_number}[PROTECT]{abbreviation}[PROTECT]{next_word}\"\n","\n","        # If the next word is lowercase or a noun, treat as non-sentence-ending\n","        # (Filtern die wirklich Bakterien etc . raus oder ist das Bauernfängerrei ?)\n","        if next_word.islower() or is_noun(next_word):\n","            return f\"{abbreviation}[PROTECT]{next_word}\"\n","\n","        # If the next word is uppercase and not a noun, treat as sentence-ending\n","        return f\"{abbreviation}[PROTECT_END]{next_word}\"\n","\n","    # Regex to match numbers and abbreviations (case-insensitive)\n","    text = re.sub(\n","        r'(\\d+\\s\\.\\s)?(\\b(?:' + '|'.join(re.escape(abbrev) for abbrev in abbreviations) + r'))\\s\\.\\s(\\w+)',\n","        abbreviation_handler,\n","        text,\n","        # Make abbreviation matching case-insensitive\n","        flags=re.IGNORECASE\n","    )\n","\n","    # Rule 7: Protect standalone characters followed by periods (abbreviations that are already split up)\n","    text = re.sub(r'\\b([a-zA-Z])\\s\\.', r'\\1[PROTECT]', text)\n","\n","    # Rule 8: Protect dynamic date formats (20 . 12 . 2020 or 20 . 12 .)\n","    text = re.sub(r'(\\d{1,2})\\s*\\.\\s*(\\d{1,2})\\s*\\.\\s*(\\d{2,4})', r'\\1[PROTECT]\\2[PROTECT]\\3', text)\n","    text = re.sub(r'(\\d{1,2})\\s*\\.\\s*(\\d{4})', r'\\1[PROTECT]\\2', text)  # Handles \"3 . 2021\"\n","\n","    # Rule 9: Protect ellipses (multiple periods after one another)\n","    text = re.sub(r'\\.\\s*\\.\\s*\\.', lambda m: m.group(0).replace('.', '[PROTECT]'), text)\n","\n","    # Rule 10: Protect periods before URL suffixes\n","    text = re.sub(\n","        r'\\.\\s*(' + '|'.join(re.escape(suffix) for suffix in url_suffixes) + r')\\b',\n","        r'[PROTECT]\\1',\n","        text,\n","        # Make matching case-insensitive\n","        flags=re.IGNORECASE\n","    )\n","\n","    return text\n","\n","# Apply to dataset with progress bar\n","data['question_text'] = data['question_text'].progress_apply(protect_patterns)\n","\n","# Check-in\n","log_data_summary(data, \"Pattern Protection\")\n","save_sample(data, \"pattern_protection\")\n"],"metadata":{"id":"ppxtFHTUtOj_","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Remove protection label and replace with period to match original punctuation\n","\n","---"],"metadata":{"id":"MaV_IJjKG8-z"}},{"cell_type":"code","source":["# Define function\n","def restore_protection(text):\n","    # Restore protected periods with single spaces\n","    text = text.replace('[PROTECT]', ' . ')\n","    text = text.replace('[PROTECT_END]', ' . ')\n","\n","    # Collapse multiple spaces into one\n","    return re.sub(r'\\s{2,}', ' ', text)"],"metadata":{"id":"F7xFEgN0GskU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Split sentences and filter for questions to then extract questions\n","\n","---"],"metadata":{"id":"_Bcm3-UytOsk"}},{"cell_type":"code","source":["# Define function\n","def extract_questions(text):\n","    # Split sentences\n","    from nltk.tokenize import sent_tokenize\n","    sentences = sent_tokenize(text)\n","\n","    # Restore protected periods\n","    sentences = [restore_protection(sentence) for sentence in sentences]\n","\n","    # Filter for questions\n","    questions = [sentence.strip() for sentence in sentences if '?' in sentence]\n","\n","    return questions"],"metadata":{"id":"V8sgx44gtOzn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Assign individual identifier to each extracted question\n","\n","---"],"metadata":{"id":"66jcZbeTtO6o"}},{"cell_type":"code","source":["# Define function\n","def assign_individual_ids(row):\n","    questions = extract_questions(row['question_text'])\n","    question_id = row['question_id']\n","\n","    return [\n","        {\n","            'question_id': question_id,\n","            # add A, B, C ... according to number of available questions\n","            'question_id_individual': f\"{question_id}__{chr(65 + i)}\",\n","            'question_text': question\n","        }\n","        for i, question in enumerate(questions)\n","    ]"],"metadata":{"id":"wIZ-Z0XltPBp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Process the entire dataset\n","---"],"metadata":{"id":"kSVlIzAptPJd"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","from tqdm import tqdm\n","\n","# Initialize tqdm for progress bar\n","tqdm.pandas(desc=\"Processing Rows\")\n","\n","# Extract questions with progress tracking\n","def process_row_with_progress(row):\n","    return assign_individual_ids(row)\n","\n","# Process rows and extend extracted questions\n","extracted_questions = []\n","for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Extracting Questions\"):\n","    extracted_questions.extend(assign_individual_ids(row))\n","\n","# Convert to DataFrame\n","extracted_df = pd.DataFrame(extracted_questions)\n","\n","# Check-in\n","log_data_summary(extracted_df, \"Question Extraction\")\n","save_sample(extracted_df, \"question_extraction\")"],"metadata":{"id":"qdNpT6IbtPcG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove duplicates one more time\n","\n","---"],"metadata":{"id":"E4xCvpRz3aZZ"}},{"cell_type":"code","source":["# Identify duplicate rows\n","duplicates = extracted_df[extracted_df.duplicated(subset='question_text', keep=False)]\n","\n","# Save duplicate rows to a file\n","duplicates.to_csv('duplicates_removed.csv', index=False, encoding='utf-8')\n","print(f\"Duplicates saved to 'duplicates_removed.csv': {len(duplicates)} rows\")\n","\n","# Remove duplicate rows from the main DataFrame\n","extracted_df.drop_duplicates(subset='question_text', inplace=True)\n","extracted_df.reset_index(drop=True, inplace=True)\n","\n","# Save a sample and log the summary after deduplication\n","save_sample(extracted_df, \"after_second_deduplication\")\n","log_data_summary(extracted_df, \"Second Deduplication\")"],"metadata":{"id":"Bplh-dhh3Hgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Save to file after final check\n","\n","---"],"metadata":{"id":"li9OWswttPja"}},{"cell_type":"code","source":["# Save to CSV\n","extracted_df.to_csv('extracted_questions.csv', index=False, encoding='utf-8')\n","print(\"Extracted questions saved to 'extracted_questions.csv'\")"],"metadata":{"id":"RQ8n4A1dtPqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","(after manual annotation)\n","* Match annotated questions with their original context for subsequent factor analysis\n","\n","---"],"metadata":{"id":"MrhaQDfgL5q9"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# File paths\n","file_1_path = \"cleaned_dataset_new.csv\"\n","file_2_path = \"representative_sample_10000(5-20).csv\"\n","output_file_path = \"representative_sample_with_context.csv\"\n","\n","# Load the datasets\n","file_1 = pd.read_csv(file_1_path, delimiter=\",\")\n","file_2 = pd.read_csv(file_2_path, delimiter=\";\")\n","\n","# Merge on \"question_id\" and add the \"question_text\" column as \"question_text_with_context\"\n","merged = file_2.merge(\n","    file_1[['question_id', 'question_text']],\n","    on=\"question_id\",\n","    how=\"left\"\n",")\n","\n","# Rename the column\n","merged.rename(columns={'question_text': 'question_text_context'}, inplace=True)\n","\n","# Save the updated file\n","merged.to_csv(output_file_path, index=False, sep=\";\")\n","print(f\"Updated file saved to '{output_file_path}'.\")"],"metadata":{"id":"z4sZBz-wMGCq"},"execution_count":null,"outputs":[]}]}