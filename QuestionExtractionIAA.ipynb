{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extract questions for inter-annotator agreement"
      ],
      "metadata": {
        "id": "HWQjvb3J_kam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Load the data set: contains token_count, Conceptual Question Type, Functional Question Type, and question_text\n",
        "\n",
        "* Group by token count: Divide token_count into 3 groups: 5-10, 11-15, and 16-20.\n",
        "\n",
        "* Extract questions for Conceptual Question Type:\n",
        "  * For each available question type in Conceptual Question Type: Extract 3 random questions from each token group (if available).\n",
        "\n",
        "* Extract Questions for Functional Question Type:\n",
        "  * For each available question type in Functional Question Type: Extract 3 random questions from each token group.\n",
        "\n",
        "* Ensure no overlap with the previously extracted questions.\n",
        "\n",
        "* Save the extracted questions to a new CSV.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TX3nWJxvwkE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq4h4rDMwggo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"updated_representative_sample.csv\"\n",
        "\n",
        "# Define token count groups\n",
        "def assign_token_group(token_count):\n",
        "    if 5 <= token_count <= 10:\n",
        "        return \"5-10\"\n",
        "    elif 11 <= token_count <= 15:\n",
        "        return \"11-15\"\n",
        "    elif 16 <= token_count <= 20:\n",
        "        return \"16-20\"\n",
        "    return None\n",
        "\n",
        "# Add token group column\n",
        "data['token_group'] = data['token_count'].apply(assign_token_group)\n",
        "\n",
        "# Function to sample questions\n",
        "def sample_questions(data, group_column, question_type_column, sample_size=3):\n",
        "    sampled_questions = []\n",
        "    grouped_data = data.groupby([question_type_column, 'token_group'])\n",
        "\n",
        "    # Iterate over each question type and token group\n",
        "    for (question_type, token_group), group in grouped_data:\n",
        "        if token_group:  # Skip rows without a valid token group\n",
        "            sample = group.sample(n=min(sample_size, len(group)), random_state=42)\n",
        "            sampled_questions.append(sample)\n",
        "\n",
        "    return pd.concat(sampled_questions).reset_index(drop=True)\n",
        "\n",
        "# Extract for \"Conceptual Question Type\"\n",
        "conceptual_questions = sample_questions(data, 'token_group', 'Conceptual Question Type')\n",
        "\n",
        "# Remove already selected questions for \"Functional Question Type\"\n",
        "remaining_data = data[~data['question_text'].isin(conceptual_questions['question_text'])]\n",
        "\n",
        "# Extract for \"Functional Question Type\"\n",
        "functional_questions = sample_questions(remaining_data, 'token_group', 'Functional Question Type')\n",
        "\n",
        "# Combine the results\n",
        "extracted_questions = pd.concat([conceptual_questions, functional_questions])\n",
        "\n",
        "# Save to a new CSV\n",
        "output_file = \"extracted_questions_for_IAA.csv\"\n",
        "extracted_questions.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Extracted questions saved to '{output_file}'.\")\n"
      ]
    }
  ]
}