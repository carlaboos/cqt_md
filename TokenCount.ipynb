{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FTUqSCJl72i23L5PRGm_YKQCFYNnJ9V4","timestamp":1734313421379}],"authorship_tag":"ABX9TyNbDEFu5n8VObCrOS9PYNlD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Compute token count distribution"],"metadata":{"id":"y3qIyMQr_x64"}},{"cell_type":"markdown","source":["---\n","\n","* Load data\n","* Load relevant packages\n","\n","---"],"metadata":{"id":"-siEUPljh5BI"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset\n","file_name = \"extracted_questions.csv\"\n","data = pd.read_csv(file_name)\n","\n","# Progress Check-In\n","print(\"\\n=== Initial Data Summary ===\")\n","print(f\"Dataset: {len(data)} rows, columns: {data.columns.tolist()}\")\n","\n","# Check the first few rows to confirm structure\n","print(data.head())"],"metadata":{"id":"rwkodEkGJMms","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load spacy\n","!pip install spacy\n","\n","# Install German language model\n","!python -m spacy download de_core_news_sm"],"metadata":{"id":"aKqXdb2jWXyJ","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Create progress check-in\n","* Create extraction of a sample set for review after each step\n","\n","---"],"metadata":{"id":"VllJbYJVZb_A"}},{"cell_type":"code","source":["# Logging functions for progress check-ins\n","def log_data_summary(data, step_name):\n","    print(f\"\\n=== Summary After Step: {step_name} ===\")\n","    print(f\"Number of rows: {len(data)}\")\n","    print(f\"Number of duplicate rows (based on 'question_text'): {data.duplicated(subset='question_text').sum()}\")\n","    print(f\"Number of empty rows in 'question_text': {data['question_text'].isnull().sum()}\")\n","    print(f\"Sample of 'question_text':\\n{data['question_text'].head(5)}\")\n","\n","def save_sample(data, step_name, sample_size=50):\n","    sample = data.sample(sample_size, random_state=42)\n","    sample.to_csv(f'sample_after_{step_name}.csv', index=False, encoding='utf-8')\n","    print(f\"Sample saved for step: {step_name}\")\n","\n","# Check-in\n","log_data_summary(data, \"Initial Load\")\n","save_sample(data, \"initial_load\")"],"metadata":{"id":"3X3-TQiVZYzg","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Compute token counts throughout data set\n","\n","---"],"metadata":{"id":"vZsN11TWQ4oN"}},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","from tqdm import tqdm\n","\n","# Load German spaCy model\n","nlp = spacy.load(\"de_core_news_sm\")\n","\n","# Function to calculate token count\n","def calculate_token_count(question):\n","    doc = nlp(question)  # Process the question with spaCy\n","    tokens = [token.text for token in doc if token.is_alpha or token.is_digit]  # Include words and numbers\n","    return len(tokens), tokens\n","\n","# Add progress bar\n","tqdm.pandas(desc=\"Processing Token Count\")\n","\n","# Apply tokenization and token count\n","data['token_count'], tokens = zip(*data['question_text'].progress_apply(calculate_token_count))\n","\n","# Save token count only for the main dataset\n","data_for_processing = data[['question_id', 'question_id_individual', 'question_text', 'token_count']]\n","data_for_processing.to_csv(\"questions_for_processing.csv\", index=False)\n","print(\"Main dataset saved to 'questions_for_processing.csv'.\")\n","\n","# Save a sample with tokens for quality management\n","sample_with_tokens = data.sample(n=1000, random_state=42)\n","sample_with_tokens['tokens'] = sample_with_tokens.index.map(lambda idx: tokens[idx])\n","sample_with_tokens.to_csv(\"sample_with_tokens.csv\", index=False)\n","print(\"Sample with tokens saved to 'sample_with_tokens.csv'.\")\n","\n","# Check-in\n","log_data_summary(data_for_processing, \"Initial Load\")\n","save_sample(data_for_processing, \"initial_load\")"],"metadata":{"id":"NPz0nLrlXOx9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Analyze token count distribution\n","* Generate summary statistics for token counts and visualise the distribution\n","\n","---"],"metadata":{"id":"KTNW1mkmLpah"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.font_manager as fm\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Read the data\n","data = pd.read_csv(\"questions_for_processing.csv\")\n","token_counts = data['token_count']\n","\n","# Specify the path to font file\n","font_path = 'lmroman10-regular.otf'\n","fm.fontManager.addfont(font_path)\n","\n","# Set the font family globally using the font's name\n","plt.rcParams['font.family'] = 'Latin Modern Roman'\n","\n","# Choose a specific color from the \"colorblind\" palette\n","my_color = sns.color_palette(\"colorblind\")[8]\n","\n","# Dynamic x-ticks: Determine x tick positions based on token_counts\n","max_ticks = 12  # Maximum number of ticks\n","x_values = token_counts.value_counts().sort_index().index  # Sorted token count values\n","step = max(1, len(x_values) // max_ticks)\n","x_ticks = range(0, x_values.max() + 1, step)\n","\n","# Create the boxplot\n","fig, ax = plt.subplots(figsize=(12, 6))\n","sns.boxplot(x=token_counts, ax=ax)\n","\n","# Set x-ticks\n","ax.set_xticks(x_ticks)\n","ax.set_xticklabels([str(i) for i in x_ticks], rotation=45, ha=\"right\", fontsize=16)\n","\n","# Ensure y-ticks are visible and set their font size (for a horizontal boxplot, there may be only one category)\n","ax.tick_params(axis='y', labelsize=16)\n","\n","# Remove lines on top and on the right\n","ax.spines[\"top\"].set_visible(False)\n","ax.spines[\"right\"].set_visible(False)\n","\n","# Set axis labels\n","ax.set_xlabel(\"Token Count\", fontsize=20, labelpad=20)\n","ax.set_ylabel(\"Frequency\", fontsize=20, labelpad=20)\n","\n","# Add vertical lines at each x-tick\n","for xtick in x_ticks:\n","    ax.axvline(x=xtick, color=\"gray\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n","\n","# Add horizontal grid lines\n","ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n","\n","# Add a horizontal line at y = 0\n","ax.axhline(y=0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n","\n","\n","# Pass color to boxplot explicitly:\n","sns.boxplot(x=token_counts, color=my_color, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Save figure\n","fig.savefig(\"TokenCountDistribution.pdf\")"],"metadata":{"id":"dHeDTWVstk-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Define which token count range to include in analysis based on mean and standard deviation\n","\n","---"],"metadata":{"id":"qkm3DJMTL2XZ"}},{"cell_type":"code","source":["# Define token count range (mean Â± standard deviation)\n","mean = token_count_stats['mean']\n","std_dev = token_count_stats['std']\n","\n","lower_bound = max(1, int(mean - std_dev))\n","upper_bound = int(mean + std_dev)\n","\n","print(f\"\\nRecommended Token Count Range: {lower_bound} to {upper_bound}\")"],"metadata":{"id":"I-TpYG-fLEJx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","* Additional token count analysis\n","\n","---"],"metadata":{"id":"uBLXKrvvtIbj"}},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import data_table\n","\n","# Load the dataset\n","file_path = \"questions_for_processing.csv\"  # Replace with your actual file path\n","data = pd.read_csv(file_path)\n","\n","# Check if the 'token_count' column exists\n","if \"token_count\" not in data.columns:\n","    raise ValueError(\"The dataset does not contain the 'token_count' column.\")\n","\n","# Calculate the distribution of token counts\n","token_count_distribution = data['token_count'].value_counts().sort_index()\n","\n","# Total number of rows for percentage calculation\n","total_rows = len(data)\n","\n","# Create a DataFrame with occurrences and percentages\n","token_count_overview = pd.DataFrame({\n","    \"Token Count\": token_count_distribution.index,\n","    \"Occurrences\": token_count_distribution.values,\n","    \"Percentage\": (token_count_distribution / total_rows) * 100\n","})\n","\n","# Reset the index for better formatting\n","token_count_overview.reset_index(drop=True, inplace=True)\n","\n","# Save the overview to a CSV file\n","output_file = \"token_count_distribution_overview.csv\"\n","token_count_overview.to_csv(output_file, index=False)\n","print(f\"\\nToken count distribution overview saved to '{output_file}'.\")\n","\n","# Print the overview\n","print(\"\\n=== Token Count Distribution Overview ===\")\n","data_table.DataTable(token_count_overview)"],"metadata":{"id":"YXA8Xa3HtO_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Choose size of dataset for labelling (token counts 5-20)\n","\n","---"],"metadata":{"id":"kNx6tahniXx5"}},{"cell_type":"code","source":["# Load the dataset\n","file_name = \"questions_for_processing.csv\"\n","data = pd.read_csv(file_name)\n","\n","# Function to sample questions by token count\n","def group_and_sample(data, sample_size_per_group, min_token_count, max_token_count):\n","    # Filter the data for token counts between min_token_count and max_token_count\n","    filtered_data = data[(data['token_count'] >= min_token_count) & (data['token_count'] <= max_token_count)]\n","\n","    # Group by token_count\n","    grouped = filtered_data.groupby('token_count')\n","    sampled_questions = []\n","    for token_count, group in grouped:\n","        size = min(sample_size_per_group, len(group))\n","        sampled_questions.append(group.sample(n=size, random_state=42))\n","    return pd.concat(sampled_questions).reset_index(drop=True)\n","\n","# Define parameters\n","sample_size_per_group = 625  # Adjust number\n","min_token_count = 5\n","max_token_count = 20\n","\n","# Use the filtered dataset (filtered_questions should be data)\n","representative_sample = group_and_sample(data, sample_size_per_group, min_token_count, max_token_count)\n","\n","print(f\"{len(representative_sample)} questions sampled between {min_token_count} and {max_token_count} tokens.\")"],"metadata":{"id":"ykUFQKvbkomQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Save to file\n","\n","---\n"],"metadata":{"id":"faXy_RZdmJLi"}},{"cell_type":"code","source":["# Save the overview to a CSV file\n","output_file = \"representative_sample_10000(5-20).csv\"\n","representative_sample.to_csv(output_file, index=False)\n","print(f\"\\nRepresentative sample saved to '{output_file}'.\")"],"metadata":{"id":"NwbRaCsMmNyN"},"execution_count":null,"outputs":[]}]}