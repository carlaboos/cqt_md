{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Carry out factor analysis"
      ],
      "metadata": {
        "id": "I8s6hU-LSDHV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKhPvWKGz0F"
      },
      "source": [
        "---\n",
        "\n",
        "* Load the dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "269ddVu4GqhD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"representative_sample_with_context_withoutDiscard.csv\", delimiter = \";\")\n",
        "\n",
        "# Progress Check-In\n",
        "print(\"\\n=== Initial Data Summary ===\")\n",
        "print(f\"Dataset: {len(data)} rows, columns: {data.columns.tolist()}\")\n",
        "\n",
        "print(\"\\n=== Dataset Preview ===\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVDRw6gPTkTB"
      },
      "source": [
        "---\n",
        "\n",
        "* Install and load relevant packages\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-8OS3gujUTWv"
      },
      "outputs": [],
      "source": [
        "# Install SpaCy for linguistic data processing\n",
        "!pip install spacy\n",
        "\n",
        "# Download the large German model\n",
        "!python -m spacy download de_core_news_lg\n",
        "\n",
        "# Install factor_analyzer for factor analysis\n",
        "!pip install factor_analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjhVy6A7d79l"
      },
      "source": [
        "---\n",
        "\n",
        "* Create new tagging rules because spaCy mislabels some words\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbjY9y88owKL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Load German spaCy model\n",
        "nlp = spacy.load(\"de_core_news_lg\")\n",
        "\n",
        "# Define retagging rules in a dictionary\n",
        "retagging_rules = {\n",
        "    # Greetings\n",
        "    \"ah\": {\"lemma\": \"ah\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ahh\": {\"lemma\": \"ah\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ahhh\": {\"lemma\": \"ah\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ahhhh\": {\"lemma\": \"ah\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ciao\": {\"lemma\": \"ciao\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ciaoi\": {\"lemma\": \"ciao\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"halli\": {\"lemma\": \"hallo\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"hallo\": {\"lemma\": \"hallo\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"hallöchen\": {\"lemma\": \"hallo\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"hello\": {\"lemma\": \"hallo\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"hey\": {\"lemma\": \"hey\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"heyy\": {\"lemma\": \"hey\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"heyyy\": {\"lemma\": \"hey\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"heyhey\": {\"lemma\": \"hey\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"hi\": {\"lemma\": \"hi\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"lg\": {\"lemma\": \"LG\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"lol\": {\"lemma\": \"lol\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"mfg\": {\"lemma\": \"MFG\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"moin\": {\"lemma\": \"moin\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"moinsen\": {\"lemma\": \"moinsen\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"oh\": {\"lemma\": \"oh\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ohh\": {\"lemma\": \"oh\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ohhh\": {\"lemma\": \"oh\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"ohhhh\": {\"lemma\": \"oh\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"tschau\": {\"lemma\": \"tschau\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"tschüss\": {\"lemma\": \"tschüss\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"tschüsschen\": {\"lemma\": \"tschüss\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"tschüssi\": {\"lemma\": \"tschüss\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "    \"tschüß\": {\"lemma\": \"tschüss\", \"pos\": \"INTJ\", \"tag\": \"ITJ\", \"morph\": \"\"},\n",
        "\n",
        "    # \"würde(s)t\" != lemma: \"würde(s)t\"\n",
        "    \"würdest\": {\"lemma\": \"werden\", \"pos\": \"AUX\", \"tag\": \"VAFIN\", \"morph\": \"Mood=Sub|Number=Sing|Person=2|Tense=Past|VerbForm=Fin\"},\n",
        "    \"würdet\": {\"lemma\": \"werden\", \"pos\": \"AUX\", \"tag\": \"VAFIN\", \"morph\": \"Mood=Sub|Number=Plur|Person=2|Tense=Past|VerbForm=Fin\"},\n",
        "\n",
        "    # \"gibts\" != VIMP\n",
        "    \"gibts\": {\"lemma\": \"geben\", \"pos\": \"VERB\", \"tag\": \"VVFIN\", \"morph\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\"},\n",
        "    \"machts\": {\"lemma\": \"machen\", \"pos\": \"VERB\", \"tag\": \"VVFIN\", \"morph\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\"},\n",
        "\n",
        "    # Abbreviations without dividing periods\n",
        "    \"eigtl\": {\"lemma\": \"eigentlich\", \"pos\": \"ADV\", \"tag\": \"ADV\", \"morph\": \"\"},\n",
        "    \"etc\": {\"lemma\": \"etc\", \"pos\": \"X\", \"tag\": \"XY\", \"morph\": \"\"},\n",
        "    \"nh\": {\"lemma\": \"nh\", \"pos\": \"X\", \"tag\": \"XY\", \"morph\": \"\"},\n",
        "    \"usw\": {\"lemma\": \"usw\", \"pos\": \"X\", \"tag\": \"XY\", \"morph\": \"\"},\n",
        "    \"vlt\": {\"lemma\": \"vielleicht\", \"pos\": \"ADV\", \"tag\": \"ADV\", \"morph\": \"\"},\n",
        "    \"vllt\": {\"lemma\": \"vielleicht\", \"pos\": \"ADV\", \"tag\": \"ADV\", \"morph\": \"\"},\n",
        "    \"zb\": {\"lemma\": \"zb\", \"pos\": \"X\", \"tag\": \"XY\", \"morph\": \"\"}\n",
        "}\n",
        "\n",
        "# Register custom attributes for tokens\n",
        "Token.set_extension(\"custom_lemma\", default=None, force=True)\n",
        "Token.set_extension(\"custom_pos\", default=None, force=True)\n",
        "Token.set_extension(\"custom_tag\", default=None, force=True)\n",
        "Token.set_extension(\"custom_morph\", default=None, force=True)\n",
        "\n",
        "\n",
        "@Language.component(\"custom_retagger\")\n",
        "def custom_retagger(doc):\n",
        "    # Retrieve the question_id from the document's user_data\n",
        "    question_id = doc.user_data.get(\"question_id\", \"unknown_id\")  # Default to \"unknown_id\" if missing\n",
        "\n",
        "    for token in doc:\n",
        "        word = token.text.lower()  # case-insensitive\n",
        "        if word in retagging_rules:\n",
        "            # Apply the custom annotations from the dictionary (override existing values)\n",
        "            rule = retagging_rules[word]\n",
        "            token._.custom_lemma = rule[\"lemma\"]\n",
        "            token._.custom_pos = rule[\"pos\"]\n",
        "            token._.custom_tag = rule[\"tag\"]\n",
        "            token._.custom_morph = rule[\"morph\"]\n",
        "            print(f\"Question ID: {question_id}, Retagged: {token.text} -> {rule}\")\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add custom retagger to the pipeline\n",
        "nlp.add_pipe(\"custom_retagger\", last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhHDhRuJzqsU"
      },
      "source": [
        "---\n",
        "* Check tagging pipeline: custom_retagger must be included (is added at the end)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FV1uqJRBRHk"
      },
      "outputs": [],
      "source": [
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAts0Hy6CCPK"
      },
      "source": [
        "---\n",
        "* Define and extract linguistic features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJEA5I_cCg39",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "\n",
        "# Ensure tqdm works with pandas' apply function\n",
        "tqdm.pandas()\n",
        "\n",
        "# Load the German SpaCy model\n",
        "#nlp = spacy.load(\"de_core_news_lg\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Define word lists for semantic analysis\n",
        "# --------------------------------------------------------------------------------\n",
        "# VERBS\n",
        "\n",
        "# Causative verbs: Expository, argumentative.\tExplains cause-effect relationships.\n",
        "causative_verbs = {\n",
        "    \"animieren\", \"anregen\", \"anspornen\", \"auslösen\", \"bedingen\", \"bewegen\", \"bewerkstelligen\", \"bewirken\", \"entfachen\", \"entfesseln\", \"entstehen\", \"ergeben\", \"ermuntern\", \"ermöglichen\", \"erregen\", \"erwecken\", \"evozieren\", \"folgen\", \"führen\", \"herbeiführen\", \"hervorrufen\", \"hinauslaufen\", \"induzieren\", \"initiieren\", \"kontrollieren\", \"lassen\", \"leiten\", \"lenken\", \"machen\", \"motivieren\", \"münden\", \"resultieren\", \"schaffen\", \"sorgen\", \"stärken\", \"triggern\", \"veranlassen\", \"verlaufen\", \"verursachen\"\n",
        "    }\n",
        "\n",
        "# Communication verbs: Conversational, narrative.\tHighlights interaction or narrative dialogue.\n",
        "communication_verbs = {\n",
        "    \"anfragen\", \"anklingeln\", \"anmailen\", \"anrufen\", \"anschreiben\", \"anschreien\", \"ansimsen\", \"smsen\", \"antexten\", \"ausplaudern\", \"aussagen\", \"babbeln\", \"begründen\", \"behaupten\", \"beichten\", \"bekanntgeben\", \"bekanntmachen\", \"bekennen\", \"benachrichtigen\", \"bereden\", \"berichten\", \"besprechen\", \"chatten\", \"debattieren\", \"diskutieren\", \"disputieren\", \"durchklingeln\", \"durchsprechen\", \"einräumen\", \"erfragen\", \"erklären\", \"erkundigen\", \"erläutern\", \"erzählen\", \"erörtern\", \"fragen\", \"gestehen\", \"grüßen\", \"hinterfragen\", \"informieren\", \"kommunizieren\", \"kontaktieren\", \"kundtun\", \"mailen\", \"melden\", \"mitteilen\", \"nachfragen\", \"nachhaken\", \"offenbaren\", \"plaudern\", \"publikmachen\", \"quasseln\", \"quatschen\", \"rufen\", \"sagen\", \"schnacken\", \"schreiben\", \"schreien\", \"simsen\", \"sprechen\", \"telefonieren\", \"texten\", \"unterhalten\", \"unterrichten\", \"verhandeln\", \"verraten\", \"verständigen\", \"veröffentlichen\", \"vorlesen\", \"wissenlassen\", \"zugeben\", \"äußern\"\n",
        "    }\n",
        "\n",
        "# Desire verbs: Reflective, persuasive.\tConveys subjectivity and personal or collective intentions.\n",
        "desire_verbs = {\n",
        "    \"abzielen\", \"anpeilen\", \"anvisieren\", \"beabsichtigen\", \"bezwecken\", \"erhoffen\", \"ersehnen\", \"erstreben\", \"ersuchen\", \"erträumen\", \"erwünschen\", \"fokussieren\", \"herbeiwünschen\", \"hoffen\", \"intendieren\", \"konzentrieren\", \"sehnen\", \"sinnen\", \"streben\", \"suchen\", \"träumen\", \"wünschen\"\n",
        "    }\n",
        "\n",
        "# Epistemic verbs: Expository, evaluative.\tMarks reasoning, belief, or uncertainty.\n",
        "epistemic_verbs = {\n",
        "    \"ahnen\", \"annehmen\", \"anzweifeln\", \"ausgehen\", \"bezweifeln\", \"erahnen\", \"erwarten\", \"klingen\", \"meinen\", \"mutmaßen\", \"rechnen\", \"scheinen\", \"schätzen\", \"spekulieren\", \"unterstellen\", \"vermuten\", \"vorausahnen\", \"vorhersehen\", \"wirken\", \"wittern\", \"zweifeln\"\n",
        "    }\n",
        "\n",
        "# Existence verbs: Descriptive, narrative. Establishes states, entities, or general conditions. (w/o sein)\n",
        "existence_verbs = {\n",
        "    \"anhalten\", \"aufhalten\", \"auftreten\", \"befinden\", \"bestehen\", \"bleiben\", \"dauern\", \"enthalten\", \"existieren\", \"fortbestehen\", \"fortdauern\", \"halten\", \"herrschen\", \"leben\", \"lieben\", \"standhalten\", \"überdauern\", \"überleben\", \"verbleiben\", \"verbringen\", \"verweilen\", \"vorkommen\", \"vorliegen\", \"weiterbestehen\", \"weitergehen\", \"wohnen\", \"währen\"\n",
        "    }\n",
        "\n",
        "# Justification verbs: Formal, argumentative. Provides reasoning or validates claims.\n",
        "justification_verbs = {\n",
        "    \"argumentieren\", \"begründen\", \"bekräftigen\", \"belegen\", \"bescheinigen\", \"bestätigen\", \"beteuern\", \"beweisen\", \"bezeugen\", \"demonstrieren\", \"einstehen\", \"legitimieren\", \"nachweisen\", \"rechtfertigen\", \"stärken\", \"stützen\", \"untermauern\", \"validieren\", \"verargumentieren\", \"vergewissern\", \"zeigen\"\n",
        "    }\n",
        "\n",
        "# Mental verbs: Reflective, analytical.\tHighlights thought processes and introspection.\n",
        "mental_verbs = {\n",
        "    \"auseinandersetzen\", \"ausklügeln\", \"beachten\", \"bedenken\", \"beherzigen\", \"berücksichtigen\", \"brüten\", \"denken\", \"durchdenken\", \"erinnern\", \"erkennen\", \"erwägen\", \"finden\", \"glauben\", \"grübeln\", \"heißen\", \"nachdenken\", \"nachgrübeln\", \"nachsinnen\", \"reflektieren\", \"sehen\",  \"sinnieren\", \"studieren\", \"überdenken\", \"überlegen\", \"vergessen\", \"verstehen\", \"wissen\"\n",
        "    }\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# ADJECTIVES\n",
        "\n",
        "# Attitudinal adjectives: Reflective, persuasive, personal. Reflect subjectivity, personal stance, and evaluation.\n",
        "attitudinal_adj = {\n",
        "    \"abartig\", \"absurd\", \"affig\", \"albern\", \"angepassrt\", \"angepisst\", \"anmaßend\", \"anständig\", \"arrogant\", \"asi\", \"asozial\", \"assi\", \"assig\", \"aufdringlich\", \"begeistert\", \"beleidigt\", \"bescheiden\", \"bescheuert\", \"beschränkt\", \"blöd\", \"blödsinnig\", \"boshaft\",  \"brav\", \"böse\", \"charmant\", \"chillig\", \"cool\", \"dankbar\", \"dienlich\", \"direkt\", \"doof\", \"dramatisch\", \"dreist\", \"dumm\", \"durchgeknallt\", \"egoistisch\", \"egozentrisch\", \"ehrlich\", \"eifersüchtig\", \"eigenartig\", \"eigensinnig\", \"eingebildet\", \"eingebildet\", \"eingeschnappt\", \"eitel\", \"engagiert\", \"enthusiastisch\", \"entspannt\", \"enttäuscht\", \"erfreut\", \"erschrocken\", \"erstaunt\", \"euphorisch\", \"extrovertiert\", \"fair\", \"falsch\", \"fantastisch\", \"fies\", \"frech\", \"freundlich\", \"frustriert\", \"förderlich\", \"geduldig\", \"geil\", \"geläufig\", \"gemein\", \"genervt\",\"gereizt\", \"geschmacklos\", \"gespentisch\", \"gewöhnlich\", \"großartig\", \"großzügig\", \"gruselig\", \"gruselig\", \"gut\", \"günstig\", \"harmonisch\", \"hasserfüllt\", \"heiter\", \"herablassend\", \"hilfreich\", \"hinterfotzig\", \"hinterhältig\", \"hirnrissig\", \"hochnäßig\", \"hoffnungsvoll\", \"höflich\", \"ichbezogen\", \"idealistisch\", \"ignorant\", \"impulsiv\", \"inkonsequent\", \"introvertiert\", \"ironisch\", \"irre\", \"kacke\", \"kindisch\", \"klasse\", \"klassisch\", \"konservativ\", \"korrekt\", \"krass\", \"lahm\", \"langweilig\", \"launisch\", \"leichtgläubig\", \"liebenswert\", \"liebenswürdig\", \"liebevoll\", \"listig\", \"locker\", \"lächerlich\", \"manisch\", \"melancholisch\", \"merkwürdig\", \"merkwürdig\", \"mies\", \"misstrauisch\", \"moralisch\",\"motiviert\", \"mutlos\", \"mysteriös\", \"nachdenklich\", \"nachlässig\", \"nachteilig\", \"naiv\", \"narzisstisch\", \"negativ\", \"neidisch\", \"neidisch\", \"nett\", \"nett\", \"neugierig\", \"normal\", \"nützlich\", \"obsessiv\", \"okay\", \"optimistisch\", \"pampig\", \"peinlich\", \"perplex\", \"pervers\", \"pessimistisch\", \"positiv\", \"provokant\", \"rational\", \"realistisch\", \"rechtens\", \"reizbar\", \"respektlos\", \"respektvoll\", \"richtig\", \"riskant\", \"rücksichtslos\", \"rücksichtsvoll\", \"sachlich\", \"schadenfroh\", \"schaurig\", \"scheiße\", \"schlau\", \"schlecht\", \"schlicht\", \"schrecklich\", \"schräg\", \"schädlich\", \"schön\", \"schüchtern\", \"selbstgefällig\", \"selbstgefällig\", \"selbstkritisch\", \"selbstverliebt\", \"selbstverliebt\", \"seltsam\", \"sentimental\", \"sicher\", \"sinnfrei\", \"sonderbar\", \"speziell\", \"spießig\", \"sprunghaft\", \"stolz\", \"stolz\", \"streng\", \"stur\", \"taktlos\", \"teuer\", \"theatralisch\", \"tolerant\", \"toll\", \"traurig\", \"töricht\", \"umgänglich\", \"unfair\", \"ungeduldig\", \"ungewöhnlich\", \"ungünstig\", \"unheimlich\", \"unhöflich\", \"unnahbar\", \"unnatürlich\", \"unnormal\", \"unruhig\", \"unsensibel\", \"unsicher\", \"unsinnig\", \"unverschämt\", \"unwichtig\", \"urig\", \"verblüfft\", \"verblüfft\", \"verdattert\", \"verletzend\", \"verletzt\", \"verliebt\", \"vermessen\", \"vernünftig\", \"verrückt\", \"verschroben\", \"versnobt\", \"vertrauensvoll\", \"vertrauenswürdig\", \"verwundert\", \"verzweifelt\", \"verärgert\", \"vorsichtig\", \"vorteilhaft\", \"weinerlich\", \"weltfremd\", \"wichtig\", \"widerlich\", \"witzig\", \"wunderlich\", \"wütend\", \"zickig\", \"zornig\", \"zufrieden\", \"zurückhaltend\", \"zwanglos\",\"zynisch\", \"öde\", \"übel\", \"übergriffig\", \"überheblich\", \"überheblich\", \"üblich\"\n",
        "    }\n",
        "\n",
        "# Descriptive adjectives: Narrative, expository, descriptive. Narrative or descriptive registers, focusing on sensory details.\n",
        "descriptive_adjectives = {\n",
        "    \"asymmetrisch\", \"bitter\", \"breit\", \"bunt\", \"dunkel\", \"durchsichtig\", \"eckig\", \"einfarbig\", \"einheitlich\", \"elastisch\", \"farbig\", \"farblos\", \"fest\", \"feucht\", \"gerade\", \"glatt\", \"groß\", \"hart\", \"hell\", \"hoch\", \"horizontal\", \"kalt\", \"kantig\", \"klar\", \"klebrig\", \"klein\", \"krumm\", \"kräftig\", \"kurz\", \"lang\", \"langsam\", \"laut\", \"leer\", \"leicht\", \"leise\", \"leise\", \"locker\", \"luftdicht\", \"matt\", \"mild\", \"nass\", \"niedrig\", \"oval\", \"pflanzlich\", \"rau\", \"rund\", \"salzig\", \"sauber\", \"sauer\", \"scharf\", \"schmal\", \"schmutzig\", \"schnell\", \"schwach\", \"schwer\", \"spitz\", \"stark\", \"staubig\", \"still\", \"stinkend\", \"stumpf\", \"symmetrisch\", \"süß\", \"tief\", \"tierisch\", \"transparent\", \"trocken\", \"trüb\", \"vertikal\", \"voll\", \"warm\", \"wasserdicht\", \"weich\", \"zäh\"\n",
        "  }\n",
        "# Modal adjectives: Instructional, evaluative.\tIndicate necessity or possibility.\n",
        "modal_adjectives = {\n",
        "    \"akzeptabel\", \"angeblich\", \"annehmbar\", \"bedenklich\", \"denkbar\", \"einleuchtend\", \"erforderlich\", \"erreichbar\", \"fraglich\", \"fragwürdig\", \"haltlos\", \"hinnehmbar\", \"hypothetisch\", \"machbar\", \"mutmaßlich\", \"möglich\", \"notwendig\", \"nötig\", \"offensichtlich\", \"plausibel\", \"problematisch\", \"realisierbar\", \"realistisch\", \"tragbar\", \"unabdingbar\", \"unabwendbar\", \"unausweichlich\", \"unbedenklich\", \"unbegründet\", \"unerlässlich\", \"ungewiss\", \"unglaubwürdig\", \"unmöglich\", \"unplausibel\", \"unsachlich\", \"unvermeidbar\", \"unvermeidlich\", \"unvertretbar\", \"unwahrscheinlich\", \"vermeidbar\", \"vermutlich\", \"verpflichtend\", \"vertretbar\", \"voraussichtlich\", \"wünschenswert\", \"zulässig\", \"zumutbar\", \"zweifelhaft\", \"zwielichtig\", \"zwingend\", \"überflüssig\"\n",
        "    }\n",
        "# Social/relational adjectives: Expository, academic.\tDescribe relationships or social roles.\n",
        "social_adjectives = {\n",
        "    \"antidemokratisch\", \"beruflich\", \"bürgerlich\", \"demokratisch\", \"diktatorisch\", \"einheimisch\", \"ethisch\", \"familiär\", \"freundschaftlich\", \"gemeinsam\", \"gemeinschaftlich\", \"gerichtlich\", \"gesellschaftlich\", \"humanitär\", \"illegal\", \"individuell\", \"interkulturell\", \"international\", \"kameradschaftlich\", \"kollegial\", \"kollektiv\", \"kooperativ\", \"kulturell\", \"legal\", \"lokal\", \"menschlich\", \"militärisch\", \"moralisch\", \"multikulturell\", \"national\", \"partnerschaftlich\", \"persönlich\", \"politisch\", \"privat\", \"rechtlich\", \"rechtmäßig\", \"regional\", \"sozial\", \"soziokulturell\", \"unkollegial\", \"unpersönlich\", \"unpolitisch\", \"unrechtmäßig\", \"unverantwortlich\", \"verantwortlich\", \"vereint\", \"wirtschaftlich\", \"zivil\", \"zwischenmenschlich\", \"öffentlich\", \"überregional\"\n",
        "    }\n",
        "\n",
        "# Scientific/technical adjectives: Academic, technical.\tDescribe technical properties.\n",
        "technical_adjectives = {\n",
        "    \"akustisch\", \"analog\", \"anorganisch\", \"biologisch\", \"chemisch\", \"deskriptiv\", \"digital\", \"dynamisch\", \"effizient\", \"elektrisch\", \"elektronisch\", \"empirisch\", \"experimentell\", \"genetisch\", \"hydraulisch\", \"ineffizient\", \"klinisch\", \"magnetisch\", \"manuell\", \"mathematisch\", \"mechanisch\", \"mikroskopisch\", \"naturwissenschaftlich\", \"normativ\", \"optisch\", \"organisch\", \"physikalisch\", \"praktisch\", \"programmierbar\", \"präzise\", \"präzise\", \"qualitativ\", \"quantitativ\", \"robust\", \"statisch\", \"statistisch\", \"technisch\", \"theoretisch\", \"thermisch\", \"ungenau\", \"unprogrammierbar\", \"wissenschaftlich\"\n",
        "    }\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# ADVERBS\n",
        "\n",
        "adverb_classes = {\n",
        "    # Possibility adverbs: subjectivity and tentative stance\n",
        "    \"adv_poss\": {\n",
        "        \"anscheinend\", \"augenscheinlich\", \"erkennbar\", \"ersichtlich\", \"eventuell\", \"gegebenenfalls\", \"höchstwahrscheinlich\", \"mutmaßlich\", \"möglicherweise\", \"offenbar\", \"offenkundig\", \"offensichtlich\", \"potenziell\", \"unwahrscheinlich\", \"vermutlich\", \"vielleicht\", \"voraussichtlich\", \"wahrscheinlich\", \"womöglich\"\n",
        "        },\n",
        "    # Place adverbs > narrative style, focusing on setting and chronology\n",
        "    \"adv_loc\": {\n",
        "        \"außen\", \"außerhalb\", \"bergab\", \"bergauf\", \"da\", \"dort\", \"draußen\", \"drinnen\", \"drüben\", \"entlang\", \"gegenüber\", \"hier\", \"hinten\", \"hinter\", \"innen\", \"irgendwo\", \"links\", \"mittendrin\", \"neben\", \"nebenan\", \"nirgendwo\", \"oben\", \"oberhalb\", \"rechts\", \"rückwärts\", \"seitlich\", \"unten\", \"unterhalb\", \"vorn\", \"vorne\", \"vorwärts\", \"woanders\", \"überall\"\n",
        "        },\n",
        "    # Time adverbs > narrative style, focusing on setting and chronology\n",
        "    \"adv_temp\": {\n",
        "        \"anfangs\", \"augenblicklich\", \"bald\", \"bereits\", \"bisher\", \"bislang\", \"damals\", \"danach\", \"dauerhaft\", \"demnächst\", \"eben\", \"einmal\", \"einst\", \"ewig\", \"früher\", \"gegenwärtig\", \"gerade\", \"gestern\", \"gleich\", \"grad\", \"grade\", \"heute\", \"heutzutage\", \"immer\", \"inzwischen\", \"irgendwann\", \"jemals\", \"jetzt\", \"künftig\", \"kürzlich\", \"lange\", \"letztendlich\", \"letztens\", \"manchmal\", \"mehrmals\", \"meist\", \"meistens\", \"mittlerweile\", \"momentan\", \"morgen\", \"nachher\", \"nachträglich\", \"neulich\", \"nie\", \"niemals\", \"noch\", \"nun\", \"oft\", \"oftmals\", \"plötzlich\", \"schlussendlich\", \"schon\", \"seitdem\", \"seither\", \"selten\", \"soeben\", \"sofort\", \"später\", \"stets\", \"unentwegt\", \"vorgestern\", \"vorher\", \"vorhin\", \"weiterhin\", \"wieder\", \"zeitlebens\", \"zeitweise\", \"zukünftig\", \"übermorgen\"\n",
        "        },\n",
        "    # linking adverbials > logical structuring and cohesion, typical in expository or formal texts\n",
        "    \"adv_link\": {\n",
        "        \"allerdings\", \"andererseits\", \"anschließend\", \"anstatt\", \"ausdrücklich\", \"außerdem\", \"dadurch\", \"daher\", \"daneben\", \"daraufhin\", \"darum\", \"darüber\", \"dazu\", \"demnach\", \"dennoch\", \"deshalb\", \"dessen\", \"deswegen\", \"diesem\", \"ebenfalls\", \"ebenso\", \"einerseits\", \"ergänzend\", \"ferner\", \"folglich\", \"gleichwohl\", \"hierbei\", \"hinaus\", \"indes\", \"insbesondere\", \"inzwischen\", \"jedoch\", \"mittlerweile\", \"obendrein\", \"schließlich\", \"sodann\", \"somit\", \"stattdessen\", \"trotzdem\", \"vielmehr\", \"weiterhin\", \"währenddessen\", \"zugleich\", \"zusätzlich\", \"zwischenzeitlich\", \"überdies\"\n",
        "        }\n",
        "}\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# PREPOSITIONS\n",
        "\n",
        "preposition_classes = {\n",
        "    # Local and temporal prepositions: Explore narrative flow and chronological organization + Highlight spatial relationships and descriptive elements.\n",
        "    \"prep_loc_temp\" : {\n",
        "        \"ab\", \"an\", \"auf\", \"aus\", \"außerhalb\", \"bei\", \"binnen\", \"bis\", \"entfernt\", \"entlang\", \"hinter\", \"in\", \"innerhalb\", \"nach\", \"nachdem\", \"nahe\", \"neben\", \"ob\", \"oberhalb\", \"seit\", \"unter\", \"unterhalb\", \"von\", \"vor\", \"während\", \"zu\", \"zwischen\", \"über\"\n",
        "        },\n",
        "\n",
        "    # Modal prepositions: Reveal manner, means, and instrumentality in text.\n",
        "    \"prep_mod\" : {\n",
        "        \"abgesehen\", \"anhand\", \"anstatt\", \"ausgenommen\", \"ausschließlich\", \"außer\", \"bezüglich\", \"betreffend\", \"betreffs\", \"durch\", \"für\", \"einschließlich\", \"entgegen\", \"entsprechend\", \"exklusive\", \"gegen\", \"gegenüber\", \"gemäß\", \"hinsichtlich\", \"inklusive\", \"laut\", \"mit\", \"mithilfe\", \"mitsamt\", \"ohne\", \"per\", \"statt\", \"via\", \"vorausgesetzt\", \"wider\", \"zugunsten\", \"zuliebe\", \"zuzüglich\"\n",
        "        },\n",
        "\n",
        "    # Causal prepositions: Analyze reasoning, cause-effect relationships.\n",
        "    \"prep_caus\" : {\n",
        "        \"angesichts\", \"anlässlich\", \"aufgrund\", \"dank\", \"halber\", \"infolge\", \"mangels\", \"trotz\", \"obwohl\", \"um\", \"unbeschadet\", \"ungeachtet\", \"wegen\", \"zu\", \"zwecks\"\n",
        "        }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Define function to extract linguistic features\n",
        "\n",
        "# functional features are based on pos-tags\n",
        "# semantic features are based on word lists\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Initialize a list to store tokenized and tagged data (save tagged output in file for future reference)\n",
        "token_data = []\n",
        "\n",
        "\n",
        "def extract_features(row):\n",
        "    # Get the text and metadata\n",
        "    question_context = row[\"question_context\"]\n",
        "    question_id = row[\"question_id\"]\n",
        "    question_id_individual = row[\"question_id_individual\"]\n",
        "    conceptual_question_type = row[\"Conceptual Question Type\"]\n",
        "    functional_question_type = row[\"Functional Question Type\"]\n",
        "    question_individual = row[\"question_individual\"]\n",
        "\n",
        "    # Create a SpaCy Doc object and attach the question_id\n",
        "    doc = nlp.make_doc(question_context)  # Only tokenize initially\n",
        "    doc.user_data[\"question_id\"] = question_id  # Attach question_id to doc.user_data\n",
        "    doc = nlp(doc)  # Process through the pipeline\n",
        "\n",
        "    # Store tokenized and tagged information\n",
        "    for token in doc:\n",
        "        token_data.append({\n",
        "            \"question_id\": question_id,\n",
        "            \"question_id_individual\": question_id_individual,\n",
        "            \"Conceptual Question Type\": conceptual_question_type,\n",
        "            \"Functional Question Type\": functional_question_type,\n",
        "            \"question_context\": question_context,\n",
        "            \"question_individual\": question_individual,\n",
        "            \"token\": token.text,\n",
        "            \"lemma\": token._.custom_lemma or token.lemma_,\n",
        "            \"pos\": token._.custom_pos or token.pos_,\n",
        "            \"tag\": token._.custom_tag or token.tag_,\n",
        "            \"morph\": token._.custom_morph or token.morph\n",
        "        })\n",
        "\n",
        "\n",
        "    # Calculate counts from semantic classes first to then be able to substract them: Adverbs\n",
        "    adv_specific = sum(1 for token in doc if (\n",
        "        token._.custom_tag or token.tag_) == \"ADV\" and any((\n",
        "            token._.custom_lemma or token.lemma_) in adverb_classes[key] for key in adverb_classes))\n",
        "    # Calculate counts from semantic classes first to then be able to substract them: Prepositions\n",
        "    prep_specific = sum(1 for token in doc if (\n",
        "        token._.custom_tag or token.tag_) in {\"APPR\", \"APPO\", \"APZR\"} and any((\n",
        "            token._.custom_lemma or token.lemma_) in preposition_classes[key] for key in preposition_classes))\n",
        "\n",
        "\n",
        "    features = {\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "        \"word_count\": len(doc),  # Total number of tokens in the line\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# NOUNS\n",
        "        # Common nouns: General information density > emphasis on objects, concepts, or categories rather than specific entities. Common in expository, descriptive, or argumentative texts, such as news articles or academic writing.\n",
        "        \"nn_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"PROPN\", \"NOUN\"} and (\n",
        "                token._.custom_tag or token.tag_) == \"NN\"),\n",
        "        # Proper nouns: Specificity and Personalization > focus on specific entities, like people, places, organizations, or events. Common in narrative or biographical texts, where storytelling or real-life examples dominate.\n",
        "        \"ne_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"PROPN\", \"NOUN\"} and (\n",
        "                token._.custom_tag or token.tag_) == \"NE\"),\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# ARTICLES\n",
        "        # Definite articles: focus on shared knowledge, cohesion, and established references >> academic, narrative, and procedural\n",
        "        \"art_def_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"DET\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ART\" and any(\n",
        "                definite == \"Def\" for definite in token.morph.get(\"Definite\", []))),\n",
        "        # Indefinite articles: exploratory or descriptive tendencies + introducing new information >> creative, conversational, or expository texts\n",
        "        \"art_indef_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"DET\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ART\" and any(\n",
        "                definite == \"Ind\" for definite in token.morph.get(\"Definite\", []))),\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# VERBS\n",
        "        # All verbs:\n",
        "          # Temporal framing (present-focused vs. past-focused).\n",
        "          # Text type and register (e.g., narrative vs. expository).\n",
        "        # Present tense, indicative mood: narrative, description, actions and events, real time, Reflects hypothetical or ongoing possibilities, obligations, or abilities. Often used in constructing present perfect tenses. >> involvement, interaction, and narrativity\n",
        "        \"v_pres_ind_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVFIN\", \"VAFIN\", \"VMFIN\"} and any(\n",
        "                    tense == \"Pres\" for tense in token.morph.get(\"Tense\", [])) and any(\n",
        "                        mood == \"Ind\" for mood in token.morph.get(\"Mood\", []))),\n",
        "        # Present tense, subjunctive mood (Konjunktiv I): reported speech and indirect discourse >> formal or journalistic style, polite or neutral tone, emphasis on objectivity and detachment\n",
        "        \"v_pres_sub_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVFIN\", \"VAFIN\", \"VMFIN\"} and any(\n",
        "                    tense == \"Pres\" for tense in token.morph.get(\"Tense\", [])) and any(\n",
        "                        mood == \"Sub\" for mood in token.morph.get(\"Mood\", []))),\n",
        "        # Past tense, indicative mood: description of past events, narrativity, hypothetical situations or past obligations. past perfect or passive constructions.\n",
        "        \"v_past_ind_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVFIN\", \"VAFIN\", \"VMFIN\"} and any(\n",
        "                    tense == \"Past\" for tense in token.morph.get(\"Tense\", [])) and any(\n",
        "                        mood == \"Ind\" for mood in token.morph.get(\"Mood\", []))),\n",
        "        # Past tense, subjunctive mood (Konjunktiv II): hypotheticals, counterfactual scenarios, or wishful thinking >> speculative or emotional tone, common in reflective or argumentative texts, softening statements, hypotheticals, or politeness, formality + higher complexity + condensing information\n",
        "        \"v_past_sub_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVFIN\", \"VAFIN\", \"VMFIN\"} and any(\n",
        "                    tense == \"Past\" for tense in token.morph.get(\"Tense\", [])) and any(\n",
        "                        mood == \"Sub\" for mood in token.morph.get(\"Mood\", []))),\n",
        "\n",
        "\n",
        "        # Infinitive + Particle \"zu\" before infinitives: formality + higher complexity + condensing information (often appear with subordinate clauses) + instructional/directive >> informational density and formality\n",
        "        \"v_inf_count\": sum(1 for token in doc if (\n",
        "            (token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVINF\", \"VMINF\", \"VAINF\", \"VVIZU\"}) or (\n",
        "                    token._.custom_pos or token.pos_) == \"PART\" and (\n",
        "                        token._.custom_tag or token.tag_) == \"PTKZU\"),\n",
        "\n",
        "\n",
        "        # Verbs in perfect forms\n",
        "        # VVPP: narrative: past events, VAPP: formal exposition or detailed procedural descriptions, VMPP: hypothetical or speculative discourse\n",
        "        \"v_pp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"AUX\", \"VERB\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"VVPP\", \"VAPP\", \"VMPP\"}),\n",
        "\n",
        "\n",
        "        # Verb lists (see above)\n",
        "        \"v_caus_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in causative_verbs),\n",
        "        \"v_comm_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in communication_verbs),\n",
        "        \"v_desire_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in desire_verbs),\n",
        "        \"v_epist_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in epistemic_verbs),\n",
        "        \"v_exist_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in existence_verbs),\n",
        "        \"v_justif_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in justification_verbs),\n",
        "        \"v_mental_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in mental_verbs),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# ADJECTIVES\n",
        "        # Attributive adjectives(spacy assigns token.pos_ == \"ADJ\" to token.tag_ == \"ADJD\")\n",
        "        # Positive: Indicates neutral descriptions, often seen in narratives, descriptive prose, or scientific texts describing phenomena (e.g., eine hohe Temperatur).\n",
        "        \"adja_pos_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADJ\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJA\" and any(\n",
        "                    degree == \"Pos\" for degree in token.morph.get(\"Degree\", []))),\n",
        "        # Comparative: Suggests a comparative focus, typical in evaluative or analytical texts.\n",
        "        \"adja_cmp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADJ\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJA\" and any(\n",
        "                    degree == \"Cmp\" for degree in token.morph.get(\"Degree\", []))),\n",
        "        # Superlative: Often used for emphasis or ranking, seen in promotional language, reviews, or advertising.\n",
        "        \"adja_sup_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADJ\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJA\" and any(\n",
        "                    degree == \"Sup\" for degree in token.morph.get(\"Degree\", []))),\n",
        "\n",
        "\n",
        "        # Adverbial adjectives (spacy assigns token.pos_ == \"ADV\" to token.tag_ == \"ADJD\")\n",
        "        # Positive: Indicates evaluation or descriptions of states, common in spoken language, dialogues, or personal narratives.\n",
        "        \"adjd_pos_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADV\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJD\" and any(\n",
        "                    degree == \"Pos\" for degree in token.morph.get(\"Degree\", []))),\n",
        "        # Comparative: Reflects relative assessments, seen in conversational comparisons or personal judgments.\n",
        "        \"adjd_cmp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADV\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJD\" and any(\n",
        "                    degree == \"Cmp\" for degree in token.morph.get(\"Degree\", []))),\n",
        "        # Superlative: Indicates strong evaluations, often found in rhetorical or persuasive contexts.\n",
        "        \"adjd_sup_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADV\" and (\n",
        "                token._.custom_tag or token.tag_) == \"ADJD\" and any(\n",
        "                    degree == \"Sup\" for degree in token.morph.get(\"Degree\", []))),\n",
        "\n",
        "\n",
        "        # Adjective lists (see above)\n",
        "        \"adj_attit_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in attitudinal_adj and (\n",
        "                token._.custom_tag or token.tag_) in {\"ADJA\", \"ADJD\"}),\n",
        "        \"adj_descr_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in descriptive_adjectives and (\n",
        "                token._.custom_tag or token.tag_) in {\"ADJA\", \"ADJD\"}),\n",
        "        \"adj_mod_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in modal_adjectives and (\n",
        "                token._.custom_tag or token.tag_) in {\"ADJA\", \"ADJD\"}),\n",
        "        \"adj_soc_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in social_adjectives and (\n",
        "                token._.custom_tag or token.tag_) in {\"ADJA\", \"ADJD\"}),\n",
        "        \"adj_tech_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in technical_adjectives and (\n",
        "                token._.custom_tag or token.tag_) in {\"ADJA\", \"ADJD\"}),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# ADVERBS\n",
        "        # Adverbs from previously defined adverb classes\n",
        "        \"adv_specific\": adv_specific,\n",
        "        # Adverbs minus previously defined adverb classes\n",
        "        \"adv_general_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"ADV\") - adv_specific,\n",
        "        # Specific adverb classes\n",
        "        \"adv_poss_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in adverb_classes[\"adv_poss\"] and (\n",
        "                token._.custom_tag or token.tag_) == \"ADV\"),\n",
        "        \"adv_loc_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in adverb_classes[\"adv_loc\"] and (\n",
        "                token._.custom_tag or token.tag_) == \"ADV\"),\n",
        "        \"adv_temp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in adverb_classes[\"adv_temp\"] and (\n",
        "                token._.custom_tag or token.tag_) == \"ADV\"),\n",
        "        \"adv_link_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in adverb_classes[\"adv_link\"] and (\n",
        "                token._.custom_tag or token.tag_) == \"ADV\"),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# PREPOSITIONS AND OTHERS\n",
        "        # Prepositions > formality and information density\n",
        "        # Prepositions from previously defined preposition classes\n",
        "        \"prep_specific\": prep_specific,\n",
        "        # Prepositions minus previously defined preposition classes\n",
        "        \"prep_general_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) in {\"APPR\", \"APPO\", \"APZR\"}) - prep_specific,\n",
        "        # Specific preposition classes\n",
        "        \"prep_loc_temp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in preposition_classes[\"prep_loc_temp\"] and (\n",
        "                token._.custom_tag or token.tag_) in {\"APPR\", \"APPO\", \"APZR\"}),\n",
        "        \"prep_mod_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in preposition_classes[\"prep_mod\"] and (\n",
        "                token._.custom_tag or token.tag_) in {\"APPR\", \"APPO\", \"APZR\"}),\n",
        "        \"prep_caus_count\": sum(1 for token in doc if (\n",
        "            token._.custom_lemma or token.lemma_) in preposition_classes[\"prep_caus\"] and (\n",
        "                token._.custom_tag or token.tag_) in {\"APPR\", \"APPO\", \"APZR\"}),\n",
        "\n",
        "        # Contractions > real time production > intimacy\n",
        "        \"prep_contra_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"APPRART\"),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# PRONOUNS: referring to shared personal knowledge + real time production\n",
        "\n",
        "        # Demonstrative pronouns: Structured, cohesive Writing, adds emphasis in informal texts\n",
        "        \"pron_dem_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"DET\", \"PRON\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"PDS\", \"PDAT\"}),\n",
        "\n",
        "        # Indefinite pronouns: use in abstract or argumentative texts signals generalizations; in diary-like or reflective texts to express uncertainty\n",
        "        \"pron_ind_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) in {\"DET\", \"PRON\"} and (\n",
        "                token._.custom_tag or token.tag_) in {\"PIS\", \"PIAT\", \"PIDAT\"}),\n",
        "\n",
        "        # Personal pronouns + reflexive personal pronouns\n",
        "        # 1st + 2nd person > Conversational or interactive registers (e.g., dialogues, speeches). + Personal or informal texts (e.g., letters, blogs)\n",
        "        \"pron_pers_1_2_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"PRON\" and (\n",
        "                token._.custom_tag or token.tag_) in {\"PPER\", \"PRF\"} and any(\n",
        "                    p in {\"1\", \"2\"} for p in token.morph.get(\"Person\", []))),\n",
        "        # 3rd person > Narrative or descriptive registers (e.g., fiction, historical accounts). Formal or impersonal texts (e.g., academic writing).\n",
        "        \"pron_pers_3_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"PRON\" and (\n",
        "                token._.custom_tag or token.tag_) in {\"PPER\", \"PRF\"} and any(\n",
        "                    p == \"3\" for p in token.morph.get(\"Person\", []))),\n",
        "\n",
        "        # Possessiv pronouns\n",
        "        # Personal writing: Diaries, blogs, and letters: personal tone, interaction, or subjective involvement.\n",
        "        # Add \"DET\" to exlcude mislabelled verb forms of \"meinen\": Token: meine, Lemma: mein, Pos: VERB\n",
        "        \"pron_poss_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"DET\" and (\n",
        "                token._.custom_tag or token.tag_) in {\"PPOSS\", \"PPOSAT\"} and any(\n",
        "                    p == \"Yes\" for p in token.morph.get(\"Poss\", []))),\n",
        "\n",
        "        # Relativ pronouns > explanatory / expository + elaborating information\n",
        "        \"pron_rel_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"PRON\" and (\n",
        "                token._.custom_tag or token.tag_) in {\"PRELS\", \"PRELAT\"}),\n",
        "\n",
        "        # Pronominal adverb: dafür, dabei, deswegen, trotzdem > justifying/explaining > purpose-showing; provide cohesion in logical arguments\n",
        "        # for some reason, spacy matches with PROAV instead of PAV\n",
        "        \"pron_adv_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"ADV\" and (\n",
        "                token._.custom_tag or token.tag_) == \"PROAV\"),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# CONJUNCTIONS / PARTICLES\n",
        "\n",
        "        # Subordinating conjunctions: indicate syntactically complex texts with multiple layers of ideas, reveal a focus on logical relations (cause-effect, conditions, etc.). Common in analytical or argumentative texts, academic writing, technical texts, and formal prose.\n",
        "        \"conj_sub_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"SCONJ\" and (\n",
        "                token._.custom_tag or token.tag_) in {\"KOUI\", \"KOUS\"}),\n",
        "        # coordinating conjunctions: spoken language and informal texts, conveying clear and straightforward relationships, typical for conversational registers and narrative or instructional texts.\n",
        "        \"conj_coor_count\": sum(1 for token in doc if (\n",
        "            token._.custom_pos or token.pos_) == \"CCONJ\" and (\n",
        "                token._.custom_tag or token.tag_) == \"KON\"),\n",
        "        # Comparative particle: Descriptive: evaluative writing, comparative analysis, or descriptive texts. Interpersonal: conversational texts or spoken language. Formal: paired with subordinating conjunctions (als ob, wie wenn), contribute to syntactic complexity in formal texts\n",
        "        \"conj_comp_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"KOKOM\"),\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# OTHERS\n",
        "\n",
        "        # Interjections > informal, conversational tone\n",
        "        \"interj_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"ITJ\"),\n",
        "\n",
        "        # WH-words > information-seeking, interactivity\n",
        "        \"wh_count\": sum(1 for token in doc if (\n",
        "            (token._.custom_pos or token.pos_) == \"PRON\" and (\n",
        "                token._.custom_tag or token.tag_) == \"PWS\") or (\n",
        "                (token._.custom_pos or token.pos_) == \"DET\" and (\n",
        "                    token._.custom_tag or token.tag_) == \"PWAT\") or (\n",
        "                    (token._.custom_pos or token.pos_) in {\"CCONJ\", \"ADV\"} and (\n",
        "                        token._.custom_tag or token.tag_) == \"PWAV\")),\n",
        "\n",
        "        # Response particles: conversational, interactive; involvement and speaker stance\n",
        "        \"resp_part_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"PTKANT\"),\n",
        "\n",
        "        # Modellkennungen / Special characters >> specific vocabulary / technical\n",
        "        \"spec_char_count\": sum(1 for token in doc if (\n",
        "            token._.custom_tag or token.tag_) == \"XY\")\n",
        "\n",
        "\n",
        "    }\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "# After processing all rows, convert token_data to a DataFrame and save it\n",
        "token_df = pd.DataFrame(token_data)\n",
        "token_df.to_csv(\"tagged_tokens_data.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Extract features for each line and convert them to a DataFrame (+ Progress bar)\n",
        "features = data.progress_apply(extract_features, axis=1).apply(pd.Series)\n",
        "\n",
        "# Combine features with the original dataset\n",
        "data = pd.concat([data, features], axis=1)\n",
        "\n",
        "# Save the enriched dataset to inspect later\n",
        "data.to_csv(\"enriched_data.csv\", index=False)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Save pos-tagged data to file for future reference\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MBBAIdK52JOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check structure\n",
        "print(type(token_data))  # Should be a list\n",
        "print(len(token_data))  # Should not be 0\n",
        "print(token_data[:5])  # Print first 5 entries\n",
        "\n",
        "if len(token_data) > 0 and isinstance(token_data[0], dict):\n",
        "    token_df = pd.DataFrame(token_data)  # Convert to DataFrame\n",
        "    print(token_df.shape)  # Check if it's still empty\n",
        "else:\n",
        "    print(\"token_data is either empty or incorrectly structured.\")\n",
        "\n",
        "print(\"Columns in token_df:\", token_df.columns.tolist())\n",
        "\n",
        "# Save to file\n",
        "token_df.to_csv(\"tagged_tokens_data.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved successfully.\")"
      ],
      "metadata": {
        "id": "9UQ_7nQGq7Ug",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Extract features to review pos-tagging (only for quality management)\n",
        "---"
      ],
      "metadata": {
        "id": "Hs5hUzrReFZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import spacy\n",
        "\n",
        "# Function to debug matches for a specific feature across all rows, including question_id\n",
        "def debug_all_to_file(data, output_file_path):\n",
        "    \"\"\"\n",
        "    Debug function to write matches for a feature rule for all rows,\n",
        "    including the question_id, into a file.\n",
        "    Parameters:\n",
        "        data: The DataFrame containing text data and question_id.\n",
        "        output_file_path: The path to the output file.\n",
        "    \"\"\"\n",
        "    total_matches = 0  # Counter for total matches\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        for index, row in data.iterrows():\n",
        "            text = row[\"question_context\"]  # column for feature extraction\n",
        "            question_id = row[\"question_id\"]  # question_id for documentation\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Find matches based on the rule\n",
        "            matches = [\n",
        "                (token.text, token._.custom_lemma or token.lemma_,\n",
        "                 token._.custom_pos or token.pos_,\n",
        "                 token._.custom_tag or token.tag_,\n",
        "                 token._.custom_morph or token.morph)\n",
        "                for token in doc\n",
        "                if (token._.custom_tag or token.tag_) == \"XY\"\n",
        "                #if (token._.custom_tag or token.tag_) in {\"PWS\", \"PWAT\", \"PWAV\"}\n",
        "            ]\n",
        "\n",
        "            # Increment total matches count\n",
        "            total_matches += len(matches)\n",
        "\n",
        "            # Write matches if any are found\n",
        "            if matches:\n",
        "                file.write(f\"Question ID: {question_id}\\n\")\n",
        "                file.write(f\"Row {index}:\\n\")\n",
        "                for match in matches:\n",
        "                    file.write(f\"Token: {match[0]}, Lemma: {match[1]}, POS: {match[2]}, Tag: {match[3]}, Morph: {match[4]}\\n\")\n",
        "                file.write(f\"Original text: {text}\\n\")\n",
        "                file.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "        # Write total number of matches\n",
        "        file.write(f\"Total matches found: {total_matches}\\n\")\n",
        "\n",
        "# Run the function for all rows in the dataset\n",
        "debug_all_to_file(data, \"Tokens with tag 'XY'_afterRetagging_250128.txt\")\n",
        "'''"
      ],
      "metadata": {
        "id": "dyqjcITQkh3X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Compare word counts with Conceptional Question Types\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SVGK6eHvIZou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set pandas options to display entire width of output\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Ensure 'word_count' is numeric\n",
        "data['word_count'] = pd.to_numeric(data['word_count'], errors='coerce')\n",
        "data = data.dropna(subset=['word_count'])  # Remove missing values\n",
        "\n",
        "# Group by 'Conceptual Question Type' and calculate summary statistics\n",
        "grouped_stats = data.groupby('Conceptual Question Type')['word_count'].describe()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Descriptive Statistics of Word Count by Conceptual Question Type:\")\n",
        "print(grouped_stats)\n",
        "\n",
        "\n",
        "# Boxplot to visualize distribution\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='Conceptual Question Type', y='word_count', data=data)\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
        "plt.xlabel(\"Conceptual Question Type\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "#plt.title(\"Distribution of Text length by Conceptual Question Type\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Reset pandas display options to default if necessary\n",
        "pd.reset_option('display.width')\n",
        "pd.reset_option('display.max_colwidth')"
      ],
      "metadata": {
        "id": "w-h1b1NRIfTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Set pandas options to display entire width of output\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# --- Data Preparation ---\n",
        "# Ensure 'word_count' is numeric and drop missing values\n",
        "data['word_count'] = pd.to_numeric(data['word_count'], errors='coerce')\n",
        "data = data.dropna(subset=['word_count'])\n",
        "\n",
        "# Group by 'Conceptual Question Type' and print summary statistics\n",
        "grouped_stats = data.groupby('Conceptual Question Type')['word_count'].describe()\n",
        "print(\"Descriptive Statistics of Word Count by Conceptual Question Type:\")\n",
        "print(grouped_stats)\n",
        "\n",
        "# --- Font and Palette Setup ---\n",
        "# Specify and add the font file (adjust path as needed)\n",
        "font_path = 'lmroman10-regular.otf'\n",
        "fm.fontManager.addfont(font_path)\n",
        "plt.rcParams['font.family'] = 'Latin Modern Roman'\n",
        "\n",
        "# Choose a specific color from the \"colorblind\" palette\n",
        "my_color = sns.color_palette(\"colorblind\")[8]\n",
        "\n",
        "# --- Create the Boxplot ---\n",
        "# Increase the figure height for a less stumpy look (e.g., 12x8 inches)\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Order the x-axis categories alphabetically\n",
        "order = sorted(data['Conceptual Question Type'].unique())\n",
        "\n",
        "# Create the boxplot with the given order and color\n",
        "sns.boxplot(x='Conceptual Question Type', y='word_count', data=data, color=my_color, ax=ax, order=order)\n",
        "\n",
        "# Rotate x-axis labels for readability and set their font size\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)\n",
        "\n",
        "# Set y-tick label size\n",
        "ax.tick_params(axis='y', labelsize=16)\n",
        "\n",
        "# Remove top and right spines\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "# Set axis labels with specified font sizes and padding\n",
        "ax.set_xlabel(\"Conceptual Question Type\", fontsize=20, labelpad=20)\n",
        "ax.set_ylabel(\"Word Count\", fontsize=20, labelpad=20)\n",
        "\n",
        "# Add horizontal grid lines\n",
        "ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the figure as a PDF file\n",
        "fig.savefig(\"WordCountDistribution.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "\n",
        "# Reset pandas display options to default if necessary\n",
        "pd.reset_option('display.width')\n",
        "pd.reset_option('display.max_colwidth')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OxBSBV3phEFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVdffAd2x-gu"
      },
      "source": [
        "---\n",
        "* Compute raw feature counts\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mExU6P1x9Qc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set pandas options to display all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "feature_counts = data[[\n",
        "                       # Nouns\n",
        "                       \"nn_count\", \"ne_count\",\n",
        "                       # Determiners\n",
        "                       \"art_def_count\", \"art_indef_count\",\n",
        "                       # Verbs\n",
        "                       \"v_pres_ind_count\", \"v_pres_sub_count\", \"v_past_ind_count\", \"v_past_sub_count\",\n",
        "                       \"v_inf_count\", \"v_pp_count\",\n",
        "                       \"v_caus_count\", \"v_comm_count\", \"v_desire_count\", \"v_epist_count\", \"v_exist_count\", \"v_justif_count\", \"v_mental_count\",\n",
        "                       # Adjectives\n",
        "                       \"adja_pos_count\", \"adja_cmp_count\", \"adja_sup_count\",\n",
        "                       \"adjd_pos_count\", \"adjd_cmp_count\", \"adjd_sup_count\",\n",
        "                       \"adj_attit_count\", \"adj_descr_count\", \"adj_mod_count\", \"adj_soc_count\", \"adj_tech_count\",\n",
        "                       # Adverbs\n",
        "                       \"adv_general_count\", \"adv_poss_count\", \"adv_loc_count\", \"adv_temp_count\", \"adv_link_count\",\n",
        "                       # Prepositions\n",
        "                       \"prep_general_count\", \"prep_loc_temp_count\", \"prep_mod_count\", \"prep_caus_count\", \"prep_contra_count\",\n",
        "                       # Pronouns\n",
        "                       \"pron_dem_count\", \"pron_ind_count\", \"pron_pers_1_2_count\", \"pron_pers_3_count\", \"pron_poss_count\", \"pron_rel_count\", \"pron_adv_count\",\n",
        "                       # Conjunctions / Particles\n",
        "                       \"conj_sub_count\", \"conj_coor_count\", \"conj_comp_count\",\n",
        "                       # Others\n",
        "                       \"interj_count\", \"wh_count\", \"resp_part_count\", \"spec_char_count\"]\n",
        "                      ].sum()\n",
        "\n",
        "\n",
        "# Print feature counts without truncation\n",
        "print(\"Feature Counts Across Dataset:\")\n",
        "print(feature_counts)\n",
        "\n",
        "# Reset pandas display options to default if necessary\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Compute descriptive statistics for each linguistic feature\n",
        "* Plot boxplot for visualisation\n",
        "---"
      ],
      "metadata": {
        "id": "VP56qpR1KiQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of feature columns (with the '_count' suffix)\n",
        "feature_columns = [\n",
        "    # Nouns\n",
        "    \"nn_count\", \"ne_count\",\n",
        "    # Determiners\n",
        "    \"art_def_count\", \"art_indef_count\",\n",
        "    # Verbs\n",
        "    \"v_pres_ind_count\", \"v_pres_sub_count\", \"v_past_ind_count\", \"v_past_sub_count\",\n",
        "    \"v_inf_count\", \"v_pp_count\",\n",
        "    \"v_caus_count\", \"v_comm_count\", \"v_desire_count\", \"v_epist_count\", \"v_exist_count\", \"v_justif_count\", \"v_mental_count\",\n",
        "    # Adjectives\n",
        "    \"adja_pos_count\", \"adja_cmp_count\", \"adja_sup_count\",\n",
        "    \"adjd_pos_count\", \"adjd_cmp_count\", \"adjd_sup_count\",\n",
        "    \"adj_attit_count\", \"adj_descr_count\", \"adj_mod_count\", \"adj_soc_count\", \"adj_tech_count\",\n",
        "    # Adverbs\n",
        "    \"adv_general_count\", \"adv_poss_count\", \"adv_loc_count\", \"adv_temp_count\", \"adv_link_count\",\n",
        "    # Prepositions\n",
        "    \"prep_general_count\", \"prep_loc_temp_count\", \"prep_mod_count\", \"prep_caus_count\", \"prep_contra_count\",\n",
        "    # Pronouns\n",
        "    \"pron_dem_count\", \"pron_ind_count\", \"pron_pers_1_2_count\", \"pron_pers_3_count\", \"pron_poss_count\", \"pron_rel_count\", \"pron_adv_count\",\n",
        "    # Conjunctions / Particles\n",
        "    \"conj_sub_count\", \"conj_coor_count\", \"conj_comp_count\",\n",
        "    # Others\n",
        "    \"interj_count\", \"wh_count\", \"resp_part_count\", \"spec_char_count\"\n",
        "]\n",
        "\n",
        "# Calculate descriptive statistics for each feature across all texts.\n",
        "# We compute: mean, min, max, and standard deviation.\n",
        "stats_df = data[feature_columns].agg(['mean', 'min', 'max', 'std'])\n",
        "\n",
        "# Compute the range (max - min) for each feature and add it as a new row.\n",
        "stats_df.loc['range'] = stats_df.loc['max'] - stats_df.loc['min']\n",
        "\n",
        "# Reorder the rows in the desired order: mean, min, max, range, std.\n",
        "stats_df = stats_df.loc[['mean', 'min', 'max', 'range', 'std']]\n",
        "\n",
        "# Rename the columns to drop the '_count' suffix.\n",
        "stats_df.columns = [col.replace('_count', '') for col in stats_df.columns]\n",
        "\n",
        "# Print the descriptive statistics with features as columns.\n",
        "print(\"Descriptive Statistics for Each Feature:\")\n",
        "print(stats_df)\n",
        "\n",
        "# Optionally, transpose the DataFrame so that each row corresponds to a feature.\n",
        "print(\"\\nDescriptive Statistics (Features as rows):\")\n",
        "print(stats_df.T)\n",
        "\n",
        "\n",
        "\n",
        "# Convert the data to long format for easier plotting\n",
        "data_long = data[feature_columns].melt(var_name='feature', value_name='count')\n",
        "# Remove the '_count' suffix for clarity in the plots\n",
        "data_long['feature'] = data_long['feature'].str.replace('_count', '')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='feature', y='count', data=data_long)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d_9RvbVVPsq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# --- Data Preparation ---\n",
        "# Assuming 'data' is already defined and contains your features.\n",
        "# List of feature columns (with the '_count' suffix)\n",
        "feature_columns = [\n",
        "    # Nouns\n",
        "    \"nn_count\", \"ne_count\",\n",
        "    # Determiners\n",
        "    \"art_def_count\", \"art_indef_count\",\n",
        "    # Verbs\n",
        "    \"v_pres_ind_count\", \"v_pres_sub_count\", \"v_past_ind_count\", \"v_past_sub_count\",\n",
        "    \"v_inf_count\", \"v_pp_count\",\n",
        "    \"v_caus_count\", \"v_comm_count\", \"v_desire_count\", \"v_epist_count\", \"v_exist_count\", \"v_justif_count\", \"v_mental_count\",\n",
        "    # Adjectives\n",
        "    \"adja_pos_count\", \"adja_cmp_count\", \"adja_sup_count\",\n",
        "    \"adjd_pos_count\", \"adjd_cmp_count\", \"adjd_sup_count\",\n",
        "    \"adj_attit_count\", \"adj_descr_count\", \"adj_mod_count\", \"adj_soc_count\", \"adj_tech_count\",\n",
        "    # Adverbs\n",
        "    \"adv_general_count\", \"adv_poss_count\", \"adv_loc_count\", \"adv_temp_count\", \"adv_link_count\",\n",
        "    # Prepositions\n",
        "    \"prep_general_count\", \"prep_loc_temp_count\", \"prep_mod_count\", \"prep_caus_count\", \"prep_contra_count\",\n",
        "    # Pronouns\n",
        "    \"pron_dem_count\", \"pron_ind_count\", \"pron_pers_1_2_count\", \"pron_pers_3_count\", \"pron_poss_count\", \"pron_rel_count\", \"pron_adv_count\",\n",
        "    # Conjunctions / Particles\n",
        "    \"conj_sub_count\", \"conj_coor_count\", \"conj_comp_count\",\n",
        "    # Others\n",
        "    \"interj_count\", \"wh_count\", \"resp_part_count\", \"spec_char_count\"\n",
        "]\n",
        "\n",
        "# Convert the data to long format for easier plotting\n",
        "data_long = data[feature_columns].melt(var_name='feature', value_name='count')\n",
        "# Remove the '_count' suffix for clarity in the plots\n",
        "data_long['feature'] = data_long['feature'].str.replace('_count', '')\n",
        "\n",
        "# Compute sorted order of features alphabetically\n",
        "order = sorted(data_long['feature'].unique())\n",
        "\n",
        "# --- Font and Palette Setup ---\n",
        "# Specify the path to your font file (adjust path as needed)\n",
        "font_path = 'lmroman10-regular.otf'\n",
        "fm.fontManager.addfont(font_path)\n",
        "plt.rcParams['font.family'] = 'Latin Modern Roman'\n",
        "\n",
        "# Choose a specific color from the \"colorblind\" palette\n",
        "my_color = sns.color_palette(\"colorblind\")[8]\n",
        "\n",
        "# --- Create the Boxplot ---\n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "sns.boxplot(x='feature', y='count', data=data_long, order=order, color=my_color, ax=ax)\n",
        "\n",
        "# Rotate x-axis labels for readability and set their font size\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=14)\n",
        "ax.tick_params(axis='y', labelsize=16)\n",
        "\n",
        "# Remove top and right spines for a cleaner look\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "# Set axis labels with specified font sizes and padding\n",
        "ax.set_xlabel(\"Feature\", fontsize=20, labelpad=20)\n",
        "ax.set_ylabel(\"Count\", fontsize=20, labelpad=20)\n",
        "\n",
        "# Add horizontal grid lines\n",
        "ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the figure as a PDF file\n",
        "fig.savefig(\"FeatureCountDistribution.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m3syRoNjoBov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOAXEj7ZDSl5"
      },
      "source": [
        "---\n",
        "\n",
        "* Normalize frequencies (by 100 words) to ensure comparability\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "td-po4xDOB-K"
      },
      "outputs": [],
      "source": [
        "# List of columns to exclude from normalization (easier than the other way around)\n",
        "exclude_columns = {\n",
        "    \"adv_specific\",\n",
        "    \"Conceptual Question Type\",\n",
        "    \"Functional Question Type\",\n",
        "    \"prep_specific\",\n",
        "    \"question_context\",\n",
        "    \"question_id\",\n",
        "    \"question_id_individual\",\n",
        "    \"question_individual\",\n",
        "    \"token_count\",\n",
        "    \"word_count\",  # Necessary for normalization # switch back to WORD_COUNT\n",
        "}\n",
        "\n",
        "# Identify columns to normalize\n",
        "columns_to_normalize = [col for col in data.columns if col not in exclude_columns and not col.endswith(\"_freq\")]\n",
        "\n",
        "# Filter rows with word_count > 0 to avoid division by zero\n",
        "data = data[data[\"word_count\"] > 0]\n",
        "\n",
        "# Normalize by `word_count` if frequency columns do not already exist (normalise by 100 words)\n",
        "if not any(col + \"_freq\" in data.columns for col in columns_to_normalize):\n",
        "    for col in columns_to_normalize:\n",
        "        data[col + \"_freq\"] = data[col] / data[\"word_count\"] * 100 # 100 because of mostly short texts (under 500 words)\n",
        "\n",
        "# Extract normalized frequency columns dynamically\n",
        "frequencies = [col for col in data.columns if col.endswith(\"_freq\")]\n",
        "feature_data = data[frequencies]\n",
        "\n",
        "# Output the frequency data for inspection\n",
        "print(feature_data.head())\n",
        "print(data.columns)  # Check available columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Standardize features to mean of 0 and standard deviation of 1\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vIfdikcwDdpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize feature data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(feature_data)"
      ],
      "metadata": {
        "id": "Ay7iMkHSTDIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Compute feature variances: Factor analysis requires that features have sufficient variance. If some features have zero or near-zero variance, it could cause issues during matrix decomposition.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cPGBCQ1r9gNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pandas options to display all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Calculate variance for each feature\n",
        "variances = feature_data.var()\n",
        "print(\"Feature variances:\")\n",
        "print(variances)\n",
        "\n",
        "# Check for zero or near-zero variance\n",
        "low_variance_features = variances[variances < 1e-6]\n",
        "print(\"Low-variance features:\")\n",
        "print(low_variance_features)\n",
        "\n",
        "# Reset pandas display options to default if necessary\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')"
      ],
      "metadata": {
        "id": "Zv-xyFg76Tmr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "* Remove low-variance features (if they exist)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jV0YEDBm6rOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not low_variance_features.empty:\n",
        "    feature_data = feature_data.drop(columns=low_variance_features.index)\n",
        "    scaled_data = StandardScaler().fit_transform(feature_data)"
      ],
      "metadata": {
        "id": "CbWIn7XA6rl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Plot scree plot to determine number of factors\n",
        "* Compute factor loadings\n",
        "* Carry out varimax and promax rotation to decide which one to choose\n",
        "* Compute variance explained by each factor\n",
        "---"
      ],
      "metadata": {
        "id": "jveRdxTsPK75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from factor_analyzer import FactorAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.font_manager as fm\n",
        "from IPython.display import display\n",
        "\n",
        "# Ensure scaled_data is a DataFrame (assuming scaled_data and feature_data are defined)\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=feature_data.columns)\n",
        "\n",
        "# Compute eigenvalues for the Scree Plot\n",
        "fa = FactorAnalyzer()\n",
        "fa.fit(scaled_data)\n",
        "eigenvalues, _ = fa.get_eigenvalues()\n",
        "\n",
        "# --- Font and Style Setup ---\n",
        "font_path = 'lmroman10-regular.otf'\n",
        "fm.fontManager.addfont(font_path)\n",
        "plt.rcParams['font.family'] = 'Latin Modern Roman'\n",
        "\n",
        "# --- Create the Scree Plot ---\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "# Plot eigenvalues with markers and a connecting line\n",
        "ax.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker=\"o\", linestyle=\"-\", color=\"black\")\n",
        "# Plot horizontal line at eigenvalue = 1\n",
        "ax.axhline(y=1, color=\"r\", linestyle=\"--\", label=\"Eigenvalue = 1\")\n",
        "\n",
        "# Set axis labels with larger fonts and extra padding\n",
        "ax.set_xlabel(\"Number of Factors\", fontsize=20, labelpad=20)\n",
        "ax.set_ylabel(\"Eigenvalue\", fontsize=20, labelpad=20)\n",
        "\n",
        "# Set tick label sizes\n",
        "ax.tick_params(axis=\"x\", labelsize=16)\n",
        "ax.tick_params(axis=\"y\", labelsize=16)\n",
        "\n",
        "# Add legend with adjusted font sizes\n",
        "ax.legend(fontsize=16, title_fontsize=20)\n",
        "\n",
        "# Add grid lines (dashed) on both axes\n",
        "ax.grid(axis=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Remove top and right spines for a cleaner look\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the scree plot as a PDF file\n",
        "fig.savefig(\"ScreePlot.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "# Decide number of factors (manually based on Scree Plot)\n",
        "num_factors = 7  # Adjust this based on the plot\n",
        "\n",
        "# Try both Varimax (orthogonal) and Promax (oblique)\n",
        "rotations = [\"varimax\", \"promax\"]\n",
        "for rotation in rotations:\n",
        "    print(f\"\\n=== Factor Analysis with {rotation.upper()} Rotation ===\")\n",
        "\n",
        "    fa = FactorAnalyzer(n_factors=num_factors, rotation=rotation)\n",
        "    fa.fit(scaled_data)\n",
        "\n",
        "    # Extract factor loadings\n",
        "    loadings = pd.DataFrame(\n",
        "        fa.loadings_,\n",
        "        index=scaled_data.columns,\n",
        "        columns=[f\"Factor {i+1}\" for i in range(num_factors)]\n",
        "    )\n",
        "\n",
        "    # Sort loadings by highest absolute value for readability\n",
        "    sorted_loadings = loadings.abs().max(axis=1).sort_values(ascending=False)\n",
        "    loadings = loadings.loc[sorted_loadings.index]\n",
        "\n",
        "    # Function to apply color formatting based on thresholds\n",
        "    def highlight_values(val):\n",
        "        abs_val = abs(val)\n",
        "        if abs_val >= 0.30:\n",
        "            return 'background-color: red'\n",
        "        elif abs_val >= 0.20:\n",
        "            return 'background-color: orange'\n",
        "        elif abs_val >= 0.10:\n",
        "            return 'background-color: yellow'\n",
        "        return ''\n",
        "\n",
        "    # Display loadings in Jupyter properly with color formatting\n",
        "    print(\"Factor Loadings (high loadings > 0.30 are salient):\")\n",
        "    display(loadings.style.map(highlight_values))\n",
        "\n",
        "    # Get variance explained by each factor\n",
        "    variance_explained = pd.DataFrame(\n",
        "        {\"Explained Variance\": fa.get_factor_variance()[1]},\n",
        "        index=[f\"Factor {i+1}\" for i in range(num_factors)]\n",
        "    )\n",
        "    print(\"\\nVariance Explained by Each Factor:\")\n",
        "    display(variance_explained)"
      ],
      "metadata": {
        "id": "Qd0fbt6tkOFr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Compute factor correlations\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cllUSa-FOz4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factor_correlation_matrix = pd.DataFrame(\n",
        "    fa.loadings_.T @ fa.loadings_,  # This computes the correlation matrix\n",
        "    index=[f\"Factor {i+1}\" for i in range(num_factors)],\n",
        "    columns=[f\"Factor {i+1}\" for i in range(num_factors)]\n",
        ")\n",
        "\n",
        "print(\"Factor Correlation Matrix:\")\n",
        "display(factor_correlation_matrix)"
      ],
      "metadata": {
        "id": "ONmq6vaiI3rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Create table for factor loadings\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MpOS9xgvd5Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert factor loadings into a DataFrame\n",
        "factor_loadings = pd.DataFrame(\n",
        "    fa.loadings_,\n",
        "    index=scaled_data.columns,\n",
        "    columns=[f\"Factor {i+1}\" for i in range(fa.n_factors)]\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "factor_loadings.to_csv(\"factor_loadings.csv\")\n",
        "\n",
        "print(\"Factor loadings saved as factor_loadings.csv.\")"
      ],
      "metadata": {
        "id": "pzKBrEW7SHNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "* Compute factor scores to merge with Conceptual Question Types\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VrPgvZU5D6O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set pandas options to display all rows and columns and max width\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# 1) Load data\n",
        "factor_loadings = pd.read_csv(\"factor_loadings.csv\", index_col=0)\n",
        "enriched_data = pd.read_csv(\"enriched_data.csv\")\n",
        "\n",
        "# Standardize column/index names\n",
        "factor_loadings.columns = factor_loadings.columns.str.strip()\n",
        "factor_loadings.index = factor_loadings.index.str.strip()\n",
        "enriched_data.columns = enriched_data.columns.str.strip()\n",
        "\n",
        "# Remove `_freq` suffix from factor_loadings feature names\n",
        "factor_loadings.index = factor_loadings.index.str.replace(\"_freq\", \"\", regex=False).str.strip()\n",
        "\n",
        "# 2) Identify feature columns\n",
        "feature_columns = enriched_data.columns[enriched_data.columns.get_loc(\"nn_count\"):].str.strip()\n",
        "\n",
        "# Ensure features match\n",
        "features_in_loadings = set(factor_loadings.index)\n",
        "features_in_enriched = set(feature_columns)\n",
        "common_features = list(features_in_loadings & features_in_enriched)\n",
        "if not common_features:\n",
        "    raise ValueError(\"No common features found between factor loadings and enriched data!\")\n",
        "\n",
        "factor_loadings = factor_loadings.loc[common_features]\n",
        "\n",
        "columns_to_keep = [\"question_id\", \"Conceptual Question Type\", \"question_context\", \"question_individual\", \"word_count\"] + common_features\n",
        "enriched_data = enriched_data[columns_to_keep]\n",
        "\n",
        "# 3) Standardize the features.\n",
        "# do NOT standardize question_id, question_context, or question type.\n",
        "scaler = StandardScaler()\n",
        "enriched_data[common_features] = scaler.fit_transform(enriched_data[common_features])\n",
        "\n",
        "# 4) Assign each feature to its most strongly associated factor\n",
        "threshold = 0.1\n",
        "feature_to_factor = {}\n",
        "for feature in factor_loadings.index:\n",
        "    max_factor = factor_loadings.loc[feature].abs().idxmax()\n",
        "    if abs(factor_loadings.loc[feature, max_factor]) >= threshold:\n",
        "        feature_to_factor[feature] = max_factor\n",
        "\n",
        "# 5) Create df_factor_scores with a row per question\n",
        "df_factor_scores = enriched_data.copy()  # keep original columns\n",
        "factor_names = factor_loadings.columns.tolist()\n",
        "\n",
        "# Initialize new factor columns with zero\n",
        "for f_name in factor_names:\n",
        "    df_factor_scores[f_name] = 0.0\n",
        "\n",
        "# Sum standardised feature values for their assigned factors\n",
        "for idx, row in df_factor_scores.iterrows():\n",
        "    for feature, factor in feature_to_factor.items():\n",
        "        df_factor_scores.at[idx, factor] += row[feature]\n",
        "\n",
        "\n",
        "# Filter for questions with word_count above 100\n",
        "short_questions = df_factor_scores[df_factor_scores[\"word_count\"] <= 100].copy()\n",
        "\n",
        "print(f\"Number of questions with <= 100 words: {len(short_questions)}\")\n",
        "\n",
        "\n",
        "# Example: retrieve top/bottom scoring questions for each factor\n",
        "for factor in factor_names:\n",
        "    # Sort descending to see top-scoring (highest) texts\n",
        "    sorted_desc = short_questions.sort_values(by=factor, ascending=False)\n",
        "    top = sorted_desc.head(10)\n",
        "    print(f\"=== Factor: {factor} | Top 10 (Highest Scores) ===\")\n",
        "    display(top[[\"question_id\", \"Conceptual Question Type\", \"question_context\", \"question_individual\", \"word_count\", factor]])\n",
        "\n",
        "    # Sort ascending to see bottom-scoring (lowest or negative) texts\n",
        "    sorted_asc = short_questions.sort_values(by=factor, ascending=True)\n",
        "    bottom = sorted_asc.head(50)\n",
        "    print(f\"=== Factor: {factor} | Bottom 10 (Lowest/Negative Scores) ===\")\n",
        "    display(bottom[[\"question_id\", \"Conceptual Question Type\", \"question_context\", \"question_individual\", \"word_count\", factor]])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n",
        "pd.reset_option('display.max_colwidth')\n",
        "pd.reset_option('display.width')"
      ],
      "metadata": {
        "id": "qoicb2-18PTG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify font file:\n",
        "font_path = 'lmroman10-regular.otf'\n",
        "fm.fontManager.addfont(font_path)\n",
        "\n",
        "# Set the font family globally using the font's name\n",
        "plt.rcParams['font.family'] = 'Latin Modern Roman'\n",
        "\n",
        "# Set Seaborn theme\n",
        "# sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"colorblind\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1) Group by question type and get mean factor scores\n",
        "group_means = (\n",
        "    df_factor_scores\n",
        "    .groupby(\"Conceptual Question Type\")[factor_names]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# 2) Convert to \"long\" format for Seaborn\n",
        "melted_factor_scores = group_means.melt(\n",
        "    id_vars=[\"Conceptual Question Type\"],\n",
        "    var_name=\"Factor\",\n",
        "    value_name=\"Score\"\n",
        ")\n",
        "\n",
        "# 3) Sort question types for consistent plotting\n",
        "sorted_question_types = sorted(melted_factor_scores[\"Conceptual Question Type\"].unique())\n",
        "\n",
        "# 4) Plot\n",
        "fig, ax = plt.subplots(figsize=(16, 7))\n",
        "\n",
        "sns.pointplot(\n",
        "    data=melted_factor_scores,\n",
        "    x=\"Conceptual Question Type\",\n",
        "    y=\"Score\",\n",
        "    hue=\"Factor\",\n",
        "    dodge=True,\n",
        "    markers=\"o\",\n",
        "    linestyle=\"none\",\n",
        "    order=sorted_question_types,\n",
        "    err_kws={'linewidth': 0.5},\n",
        "    linewidth=0.5,\n",
        "    markersize=12,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_ylim(-3.5, 3.5)\n",
        "\n",
        "# Add vertical lines for each category and a grid along y\n",
        "for i in range(len(sorted_question_types)):\n",
        "    plt.axvline(x=i, color=\"gray\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "# Add a horizontal line at y=0\n",
        "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "# Remove lines on top and on the right\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "plt.xticks(rotation=45, ha=\"right\", fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "\n",
        "# Labels & legend\n",
        "plt.xlabel(\"Conceptual Question Type\", fontsize=20, labelpad=20)\n",
        "plt.ylabel(\"Mean Factor Score\", fontsize=20, labelpad=20)\n",
        "plt.legend(title=\"Factors\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0, fontsize=16, title_fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure\n",
        "fig.savefig(\"factor_scores_CQT.pdf\")"
      ],
      "metadata": {
        "id": "WiNl-nnF-mGf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Create one plot per factor with question types on the y-axis, ordered by their mean score (from highest to lowest)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sJKvm5J3oDPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "for factor in factor_names:\n",
        "    # Copy and sort the data for the current factor\n",
        "    factor_data = group_means[[\"Conceptual Question Type\", factor]].copy()\n",
        "    factor_data = factor_data.sort_values(by=factor, ascending=True)\n",
        "\n",
        "    # Create a new column for bar colors:\n",
        "    factor_data[\"bar_color\"] = factor_data[factor].apply(\n",
        "        lambda x: \"#FF9741\" if x >= 0 else \"#9D76E4\"\n",
        "    )\n",
        "\n",
        "    # Create a horizontal bar plot using Matplotlib\n",
        "    fig, ax = plt.subplots(figsize=(16, 7))\n",
        "    bars = ax.barh(\n",
        "        y=factor_data[\"Conceptual Question Type\"],\n",
        "        width=factor_data[factor],\n",
        "        color=factor_data[\"bar_color\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # ADD NUMERIC LABELS AT THE END OF EACH BAR\n",
        "    # -------------------------------------------------------------------------\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()  # The length of the bar\n",
        "        y_pos = bar.get_y() + bar.get_height() / 2  # Vertical center of the bar\n",
        "\n",
        "        # Decide text alignment depending on positive or negative\n",
        "        if width >= 0:\n",
        "            ha = \"left\"   # label appears to the right of the bar end\n",
        "            offset = 0.02\n",
        "        else:\n",
        "            ha = \"right\"  # label appears to the left of the bar end\n",
        "            offset = -0.02\n",
        "\n",
        "        # Place the text slightly beyond the bar end\n",
        "        ax.text(\n",
        "            width + offset,\n",
        "            y_pos,\n",
        "            f\"{width:.2f}\",\n",
        "            va=\"center\",\n",
        "            ha=ha,\n",
        "            fontsize=12\n",
        "        )\n",
        "\n",
        "    # Add a vertical reference line at 0\n",
        "    ax.axvline(0, color=\"gray\", linewidth=1.2, linestyle=\"--\")\n",
        "\n",
        "    # Set labels and tick parameters\n",
        "    ax.set_xlabel(f\"{factor}: Factor Scores\", fontsize=20, labelpad=20)\n",
        "    ax.set_ylabel(\"Conceptual Question Type\", fontsize=20, labelpad=20)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "\n",
        "    # Add horizontal grid lines\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "    # Force the x-axis to always span from -3.5 to 3.5\n",
        "    ax.set_xlim(-3.5, 3.5)\n",
        "\n",
        "    # Remove lines on top and on the right\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # CREATE A LEGEND\n",
        "    # -------------------------------------------------------------------------\n",
        "    pos_patch = mpatches.Patch(color=\"#FF9741\", label=\"Positive Score\")\n",
        "    neg_patch = mpatches.Patch(color=\"#9D76E4\", label=\"Negative Score\")\n",
        "    ax.legend(handles=[pos_patch, neg_patch], loc=\"right\", fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save each figure with a unique filename including the factor name\n",
        "    filename = f\"factor_scores_CQT_{factor.replace(' ', '_')}.pdf\"\n",
        "    fig.savefig(filename)\n",
        "\n",
        "    # Display the figure\n",
        "    plt.show()\n",
        "\n",
        "    # Close the figure to free up memory\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "cl1dG7o6n-2z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Present factor scores in numerical format\n",
        "---"
      ],
      "metadata": {
        "id": "4jdQOIo5ypM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "# 1) Group by question type and get mean factor scores\n",
        "group_means = (\n",
        "    df_factor_scores\n",
        "    .groupby(\"Conceptual Question Type\")[factor_names]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Round to 2 decimal places for neat printing\n",
        "group_means_rounded = group_means.copy()\n",
        "group_means_rounded[factor_names] = group_means_rounded[factor_names].round(2)\n",
        "\n",
        "print(\"Mean factor scores by question type:\\n\")\n",
        "print(group_means_rounded)\n",
        "\n",
        "\n",
        "pd.reset_option('display.max_colwidth')\n",
        "pd.reset_option('display.width')"
      ],
      "metadata": {
        "id": "NEw9faBdpEF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* Use raw factor score data (before aggregation) to fit a General Linear Model + ANOVA for each factor\n",
        "* Test whether the factor scores differ significantly by question type\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YyGk8dI5yupG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "# Assume df_factor_scores is your DataFrame and factor_names is your list of factor columns.\n",
        "# Optionally, rename the question type column for easier reference:\n",
        "df = df_factor_scores.copy()\n",
        "df = df.rename(columns={\"Conceptual Question Type\": \"ConceptualQuestionType\"})\n",
        "\n",
        "# Loop over each factor to run the GLM (ANOVA)\n",
        "for factor in factor_names:\n",
        "    print(f\"\\nGeneral Linear Model for {factor}:\")\n",
        "\n",
        "    # Wrap the factor in Q() to handle spaces or special characters\n",
        "    formula = f'Q(\"{factor}\") ~ C(ConceptualQuestionType)'\n",
        "\n",
        "    # Fit the model using ordinary least squares (OLS)\n",
        "    model = smf.ols(formula, data=df).fit()\n",
        "\n",
        "    # Print a summary of the model (coefficients, p-values, etc.)\n",
        "    print(model.summary())\n",
        "\n",
        "    # Get the ANOVA table to test overall differences among groups\n",
        "    anova_results = anova_lm(model, typ=2)\n",
        "    print(\"\\nANOVA results:\")\n",
        "    print(anova_results)"
      ],
      "metadata": {
        "id": "S7BMl1QUySuY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}