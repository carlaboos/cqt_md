{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKuxCg2cnOJUjBJRX5Wm/W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pre-process and clean data"],"metadata":{"id":"jcRyY5eb_7FR"}},{"cell_type":"markdown","source":["---\n","* Load the data sets (1 + 2)\n","\n","---"],"metadata":{"id":"KBoMnFCFysIr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"55ka5Qz7wESl","collapsed":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load Dataset 1: Only the 'question_text' column is relevant\n","dataset1 = pd.read_csv('dataset1.csv', delimiter=';', encoding='utf-8', usecols=['question_text'])\n","\n","# Load Dataset 2: Only the 'question_title' and 'question_body' columns are relevant\n","dataset2 = pd.read_csv('dataset2.csv', delimiter=',', encoding='utf-8', usecols=['question_title', 'question_body'])\n","\n","# Combine relevant columns from Dataset 2\n","dataset2['question_text'] = dataset2['question_title'].fillna('') + \" \" + dataset2['question_body'].fillna('')\n","\n","# Drop the original columns after combining\n","dataset2 = dataset2[['question_text']]\n","\n","# Progress Check-In\n","print(\"\\n=== Initial Data Summary ===\")\n","print(f\"Dataset 1: {len(dataset1)} rows, columns: {dataset1.columns.tolist()}\")\n","print(f\"Dataset 2: {len(dataset2)} rows, columns: {dataset2.columns.tolist()}\")\n","\n","print(\"\\n=== Dataset 1 Preview ===\")\n","print(dataset1.head())\n","\n","print(\"\\n=== Dataset 2 Preview ===\")\n","print(dataset2.head())\n","\n"]},{"cell_type":"markdown","source":["---\n","* Create progress check-in\n","* Create extraction of a sample set for review after each step\n","\n","---"],"metadata":{"id":"Hv7GxyOO8HFW"}},{"cell_type":"code","source":["# Progress check-in\n","\n","def log_data_summary(data, step_name):\n","    print(f\"\\n=== Summary After Step: {step_name} ===\")\n","    print(f\"Number of rows: {len(data)}\")\n","    print(f\"Number of duplicate rows (based on 'question_text'): {data.duplicated(subset='question_text').sum()}\")\n","    print(f\"Number of empty rows in 'question_text': {data['question_text'].isnull().sum()}\")\n","    print(f\"Sample of 'question_text':\\n{data['question_text'].head(5)}\")\n","\n","\n","# Save a sample of 50 rows for review\n","def save_sample(data, step_name, sample_size=50):\n","    sample = data.sample(sample_size, random_state=42)\n","    sample.to_csv(f'sample_after_{step_name}.csv', index=False, encoding='utf-8')\n","    print(f\"Sample saved for step: {step_name}\")"],"metadata":{"id":"9PmMaQVD8Fxq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Concatenate data sets and handle duplicates\n","\n","---"],"metadata":{"id":"d3E_07JwwTXF"}},{"cell_type":"code","source":["# Combine datasets for de-duplication\n","combined_dataset = pd.concat([dataset1[['question_text']], dataset2[['question_text']]])\n","\n","# Remove duplicates based on 'question_text'\n","combined_dataset.drop_duplicates(subset='question_text', inplace=True)\n","\n","# Reset index after dropping duplicates\n","combined_dataset.reset_index(drop=True, inplace=True)\n","\n","# Check-in\n","save_sample(combined_dataset, \"deduplication\")\n","log_data_summary(combined_dataset, \"Deduplication - Combined Dataset\")"],"metadata":{"id":"iqMRH7HaNitC","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Add key to each question to ensure tracking and matching of questions with their context in later steps\n","\n","---"],"metadata":{"id":"r9K6VAZld3p5"}},{"cell_type":"code","source":["import uuid\n","\n","# Add a unique key column using UUIDs\n","combined_dataset['question_id'] = [str(uuid.uuid4()) for _ in range(len(combined_dataset))]\n","\n","# Save combinded dataset to a CSV file (to be able to compare with versions after cleaning steps)\n","combined_dataset.to_csv('combined_dataset.csv', index=False, encoding='utf-8')\n","print(\"Combined dataset saved to 'combined_dataset.csv'\")"],"metadata":{"id":"OqqGiEjzdxmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove HTML tags\n","\n","---"],"metadata":{"id":"bafmfNTNwReG"}},{"cell_type":"code","source":["import re\n","\n","# Define function\n","def remove_html_tags(text):\n","    return re.sub(r'<[^>]*>', '', text)\n","\n","# Apply to dataset\n","combined_dataset['question_text'] = combined_dataset['question_text'].apply(remove_html_tags)\n","\n","# Check-in\n","save_sample(combined_dataset, \"html_tag_removal\")\n","log_data_summary(combined_dataset, \"HTML Removal\")"],"metadata":{"id":"GOLIS6kDwRwd","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove URLs\n","\n","---"],"metadata":{"id":"U-cBa6nDwSRS"}},{"cell_type":"code","source":["# Define function\n","def remove_urls(text):\n","    return re.sub(r'http\\S+|www\\S+', '', text) # leave instances of \"Gutefrage\" in\n","\n","# Apply to Dataset\n","combined_dataset['question_text'] = combined_dataset['question_text'].apply(remove_urls)\n","\n","# Check-in\n","save_sample(combined_dataset, \"url_removal\")\n","log_data_summary(combined_dataset, \"URL Removal\")"],"metadata":{"id":"0aCrzQlUwSaa","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","* Normalise punctuation\n","\n","---"],"metadata":{"id":"ySX0etvQwR9O"}},{"cell_type":"code","source":["# Define function\n","def normalize_punctuation(text):\n","    # Replace underscores in numbers (e.g., 1_23 -> 1,23)\n","    #text = re.sub(r'(\\d)_+(\\d)', r'\\1,\\2', text)\n","\n","    # Replace underscores in abbreviations (e.g., z_B -> z . B )\n","    text = re.sub(r'\\b([A-Za-z])_+([A-Za-z])\\b', r'\\1 . \\2', text)\n","\n","    # Standardize quotation marks\n","    text = text.replace('“', '\"').replace('”', '\"').replace('„', '\"').replace('‚', \"'\").replace('‘', \"'\").replace('´ ´', '\"').replace('´', \"'\")\n","\n","    return text\n","\n","\n","# Apply to Dataset\n","combined_dataset['question_text'] = combined_dataset['question_text'].apply(normalize_punctuation)\n","\n","# Check-in\n","save_sample(combined_dataset, \"punctuation_normalization\")\n","log_data_summary(combined_dataset, \"Punctuation Normalization\")"],"metadata":{"id":"ih1pjfQMwSGm","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove emojis and unicode\n","\n","---"],"metadata":{"id":"TFzP1c9pwS2A"}},{"cell_type":"code","source":["# Define function\n","def remove_non_printable_and_emojis(text):\n","    # Remove zero-width characters (e.g., <0x200b>)\n","    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n","\n","    # Remove emojis (cover a wide range of emoji Unicode blocks)\n","    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002700-\\U000027BF\\U00002B50-\\U00002B55]', '', text)\n","\n","    # Remove Chinese characters (ranges: \\u4E00-\\u9FFF, \\u3400-\\u4DBF)\n","    text = re.sub(r'[\\u4E00-\\u9FFF\\u3400-\\u4DBF]', '', text)\n","\n","    # Remove non-printable control characters\n","    text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n","\n","    # Retain Latin characters, punctuation, and symbols like +, -, (, ), [, ], etc.\n","    # Allow characters commonly used in text processing\n","    text = re.sub(r'[^\\w\\s.,$%&/#@°^:;€!?äöüßÄÖÜ+\\-*/=<>()[\\]{}]', '', text)\n","\n","    return text"],"metadata":{"id":"IiMSKsNqwS-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply to Dataset\n","combined_dataset['question_text'] = combined_dataset['question_text'].apply(remove_non_printable_and_emojis)\n","\n","# Check-in\n","save_sample(combined_dataset, \"emoji_unicode_removal\")\n","log_data_summary(combined_dataset, \"Emoji and Unicode Removal\")"],"metadata":{"id":"Z0qk-WoRwTN_","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Anonymise users\n","\n","---"],"metadata":{"id":"OfLWumO2rTS_"}},{"cell_type":"code","source":["# List of common German greetings and sign-offs (case insensitive)\n","sign_off_regex = r\"(?i)((liebe(n|r)?|viele|beste(n)?|herzliche(n)?|mit freundliche(n|m)?) (grüße(n)?|gruß)|(?<!\\w)lg(?!\\w)|(?<!\\w)mfg(?!\\w)|(?<!\\w)grüße(?!\\w)|(?<!\\w)gruß(?!\\w))\""],"metadata":{"id":"WwXS2cyyrTuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define function\n","def anonymize_after_sign_off(text):\n","    # Search for the first occurrence of a sign-off pattern\n","    match = re.search(sign_off_regex, text)\n","    if match:\n","        # Preserve the part before the sign-off, append [ANONYMIZED] after the sign-off\n","        return text[:match.end()] + \" [ANONYMIZED]\"\n","    # Return the original text if no sign-off is found\n","    return text"],"metadata":{"id":"xMp2r6v2sQJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply to Dataset\n","combined_dataset['question_text'] = combined_dataset['question_text'].apply(anonymize_after_sign_off)\n","\n","# Check-in\n","save_sample(combined_dataset, \"anonymized_sign_offs\")\n","log_data_summary(combined_dataset, \"Anonymization\")"],"metadata":{"id":"LvKc4n8asYz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove non-German material\n","\n","---"],"metadata":{"id":"hOyEveVA6cH7"}},{"cell_type":"code","source":["# Install library\n","!pip install langdetect"],"metadata":{"id":"CuV5mk0V6tlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from langdetect import detect, DetectorFactory\n","from langdetect.lang_detect_exception import LangDetectException\n","\n","# Ensure consistent results\n","DetectorFactory.seed = 42\n","\n","# Define function\n","def detect_language_with_progress(text):\n","    try:\n","        # Detect the language of the text\n","        return detect(text)\n","    except LangDetectException:\n","        # Return 'unknown' if detection fails\n","        return 'unknown'\n","\n","# Add tqdm progress bar for monitoring\n","tqdm.pandas(desc=\"Detecting Language\")\n","combined_dataset['detected_language'] = combined_dataset['question_text'].progress_apply(detect_language_with_progress)\n","\n","# Filter out rows that are not in German\n","german_dataset = combined_dataset[combined_dataset['detected_language'] == 'de']\n","\n","# Save non-German rows to a file\n","non_german_dataset = combined_dataset[combined_dataset['detected_language'] != 'de']\n","non_german_dataset.to_csv('non_german_rows.csv', index=False, encoding='utf-8')\n","print(f\"Non-German rows saved to 'non_german_rows.csv'\")\n","\n","# Log results\n","print(f\"Total rows before filtering: {len(combined_dataset)}\")\n","print(f\"Rows retained (German): {len(german_dataset)}\")\n","print(f\"Rows removed (Non-German): {len(non_german_dataset)}\")\n","\n","# Drop temporary column after filtering\n","combined_dataset = german_dataset.drop(columns=['detected_language'])\n","\n","# Check-in\n","save_sample(combined_dataset, \"language_filtered\")\n","log_data_summary(combined_dataset, \"Language Filtering\")"],"metadata":{"id":"ox56IzxH6jkz","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","* Remove duplicates one more time\n","* Ensure the data set is ready for extraction and labelling\n","\n","---"],"metadata":{"id":"4qD5rS_I6krW"}},{"cell_type":"code","source":["# Remove duplicate rows\n","combined_dataset.drop_duplicates(subset='question_text', inplace=True)\n","combined_dataset.reset_index(drop=True, inplace=True)\n","\n","save_sample(combined_dataset, \"after_second_deduplication\")\n","log_data_summary(combined_dataset, \"Second Deduplication\")"],"metadata":{"id":"VfIpppNMrCt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","* Export cleaned data\n","* Save the cleaned data for further processing and labelling\n","\n","---"],"metadata":{"id":"J07_hhsKQIBS"}},{"cell_type":"code","source":["# Save the DataFrame\n","combined_dataset.to_csv('cleaned_dataset.csv', index=False, encoding='utf-8')"],"metadata":{"id":"uInXMiSQQX6r"},"execution_count":null,"outputs":[]}]}